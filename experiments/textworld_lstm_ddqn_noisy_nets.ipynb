{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textworld-lstm-ddqn-noisy-nets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaT3om3nMpml",
        "colab_type": "text"
      },
      "source": [
        "# Textworld starting kit notebook\n",
        "\n",
        "Model: *LSTM-DQN*\n",
        "\n",
        "When running first: \n",
        " 1. Run the first 2 code cells(with pip installations)\n",
        " 2. Restart runtime\n",
        " 3. Continue with the next cells\n",
        "\n",
        "This is done, because there is a problem with dependencies of **textworld** and **colab**, requiring different versions of **prompt-toolkit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QVq2c31UByC",
        "colab_type": "text"
      },
      "source": [
        "## Todo\n",
        "### RL:\n",
        "* [x] Prioritized Replay Memory\n",
        "* [x] [N-step DQN](https://www.groundai.com/project/understanding-multi-step-deep-reinforcement-learning-a-systematic-study-of-the-dqn-target/)\n",
        "* [x] [Fixed Q-targets](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/)\n",
        "* [x] [Double DQN](https://towardsdatascience.com/deep-double-q-learning-7fca410b193a)\n",
        "* [ ] Dueling DQN\n",
        "* [ ] Multiple inputs (description, inventory, quests, etc)\n",
        "* [x] Replay memory sample, when not having any alpha/beta priority samples, should take the whole sample of the opposite priority.\n",
        "* [ ] Noisy nets\n",
        "* [ ] DRQN ?\n",
        "* [ ] [Rainbow Paper](https://arxiv.org/pdf/1710.02298.pdf) ?\n",
        "\n",
        "### NLP:\n",
        "* [x] Normalize tokens\n",
        "* [x] Add l2 regularization\n",
        "* [ ] Remove apostrophes from descriptions\n",
        "* [x] Two word adjectives\n",
        "\n",
        "### Game Env:\n",
        "* [x] Train with simple games in starting kit\n",
        "* [ ] Check game generation and complexity\n",
        "* [ ] Make many simple(easy) games, each of which teaches different skill.\n",
        "* [ ] Train first with simple games, and progressively train on more complex games.\n",
        "\n",
        "### Debugging:\n",
        "* [x] Extended log for checking steps and scores on every epoch\n",
        "* [ ] Graphs with speed, reward, epsilon movement, loss function, episode length (use Tensorboard or similar)\n",
        "* [ ] Model comparisson\n",
        "\n",
        "### Colab:\n",
        "* [x] Export model parameters to Drive\n",
        "* [x] Export replay memory to Drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBFYrJbqjScT",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-XUxa37Z1S5",
        "colab_type": "code",
        "outputId": "60bd5398-f9c2-4361-a642-994923311205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        }
      },
      "source": [
        "!pip install textworld"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textworld in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pydot>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.12.3)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.6/dist-packages (from textworld) (2.3)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.0.3)\n",
            "Requirement already satisfied: tatsu>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from textworld) (4.4.0)\n",
            "Requirement already satisfied: gevent==1.3.5 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.16.4)\n",
            "Requirement already satisfied: selenium>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from textworld) (3.141.0)\n",
            "Requirement already satisfied: pyyaml>=3.12 in /usr/local/lib/python3.6/dist-packages (from textworld) (3.13)\n",
            "Requirement already satisfied: pillow>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from textworld) (6.1.0)\n",
            "Requirement already satisfied: urwid>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from textworld) (2.0.1)\n",
            "Requirement already satisfied: gym==0.10.4 in /usr/local/lib/python3.6/dist-packages (from textworld) (0.10.4)\n",
            "Requirement already satisfied: jericho>=1.1.5 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.2.4)\n",
            "Requirement already satisfied: greenlet==0.4.13 in /usr/local/lib/python3.6/dist-packages (from textworld) (0.4.13)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from textworld)\n",
            "  Using cached https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.17.1 in /usr/local/lib/python3.6/dist-packages (from textworld) (4.28.1)\n",
            "Requirement already satisfied: hashids>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.2.0)\n",
            "Requirement already satisfied: pybars3>=0.9.3 in /usr/local/lib/python3.6/dist-packages (from textworld) (0.9.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from textworld) (7.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot>=1.2.4->textworld) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0.0->textworld) (2.19)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2->textworld) (4.4.0)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->textworld) (0.15.4)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->textworld) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->textworld) (2.10.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->textworld) (1.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium>=3.12.0->textworld) (1.24.3)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.4->textworld) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.10.4->textworld) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.4->textworld) (1.3.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->textworld) (0.1.7)\n",
            "Requirement already satisfied: PyMeta3>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from pybars3>=0.9.3->textworld) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->flask>=1.0.2->textworld) (1.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.4->textworld) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.4->textworld) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.4->textworld) (2.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym==0.10.4->textworld) (0.16.0)\n",
            "\u001b[31mERROR: ipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit\n",
            "  Found existing installation: prompt-toolkit 1.0.16\n",
            "    Uninstalling prompt-toolkit-1.0.16:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.16\n",
            "Successfully installed prompt-toolkit-2.0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPpv7-6bVOsP",
        "colab_type": "code",
        "outputId": "9bfcafe6-b78c-4172-c2aa-504996500511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "!pip install prompt-toolkit==1.0.16"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting prompt-toolkit==1.0.16\n",
            "  Using cached https://files.pythonhosted.org/packages/57/a8/a151b6c61718eabe6b4672b6aa760b734989316d62ec1ba4996765e602d4/prompt_toolkit-1.0.16-py3-none-any.whl\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit==1.0.16) (0.1.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit==1.0.16) (1.12.0)\n",
            "\u001b[31mERROR: textworld 1.1.1 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.16 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jupyter-console 6.0.0 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.16 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit\n",
            "  Found existing installation: prompt-toolkit 2.0.9\n",
            "    Uninstalling prompt-toolkit-2.0.9:\n",
            "      Successfully uninstalled prompt-toolkit-2.0.9\n",
            "Successfully installed prompt-toolkit-1.0.16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqz6G9llwXOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iy_zBX_JyK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "![[ ! -f './glove.6B.zip' ]] && wget 'http://nlp.stanford.edu/data/glove.6B.zip' && unzip './glove.6B.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGhiMabSRqjz",
        "colab_type": "code",
        "outputId": "407f2a52-0a33-4206-bd76-942b1c5db4f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!head glove.6B.50d.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
            ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
            ". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n",
            "of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n",
            "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n",
            "and 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n",
            "in 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n",
            "a 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n",
            "\" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n",
            "'s 0.23727 0.40478 -0.20547 0.58805 0.65533 0.32867 -0.81964 -0.23236 0.27428 0.24265 0.054992 0.16296 -1.2555 -0.086437 0.44536 0.096561 -0.16519 0.058378 -0.38598 0.086977 0.0033869 0.55095 -0.77697 -0.62096 0.092948 -2.5685 -0.67739 0.10151 -0.48643 -0.057805 3.1859 -0.017554 -0.16138 0.055486 -0.25885 -0.33938 -0.19928 0.26049 0.10478 -0.55934 -0.12342 0.65961 -0.51802 -0.82995 -0.082739 0.28155 -0.423 -0.27378 -0.007901 -0.030231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuCM9FzGl2Zk",
        "colab_type": "code",
        "outputId": "1790a081-490d-4c83-d94d-b9038e4ba6a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import logging\n",
        "import yaml\n",
        "import math\n",
        "import copy\n",
        "import spacy\n",
        "import numpy as np\n",
        "import glob\n",
        "import pickle\n",
        "import re\n",
        "import csv\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any\n",
        "from collections import namedtuple\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gym\n",
        "import textworld.gym\n",
        "from textworld import EnvInfos\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWGX1rdFtIgW",
        "colab_type": "text"
      },
      "source": [
        "## Generic functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zO94P3hjyH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_np(x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "        return x\n",
        "    return x.data.cpu().numpy()\n",
        "\n",
        "\n",
        "def to_pt(np_matrix, enable_cuda=False, type='long'):\n",
        "    if type == 'long':\n",
        "        if enable_cuda:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.LongTensor).cuda())\n",
        "        else:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.LongTensor))\n",
        "    elif type == 'float':\n",
        "        if enable_cuda:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.FloatTensor).cuda())\n",
        "        else:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.FloatTensor))\n",
        "\n",
        "\n",
        "def _words_to_ids(words, word2id):\n",
        "    ids = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            ids.append(word2id[word])\n",
        "        except KeyError:\n",
        "            ids.append(1)\n",
        "    return ids\n",
        "\n",
        "\n",
        "def preproc(s, str_type='None', tokenizer=None, lower_case=True):\n",
        "    if s is None:\n",
        "        return [\"nothing\"]\n",
        "    s = s.replace(\"\\n\", ' ')\n",
        "    if s.strip() == \"\":\n",
        "        return [\"nothing\"]\n",
        "    if str_type == 'feedback':\n",
        "        if \"$$$$$$$\" in s:\n",
        "            s = \"\"\n",
        "        if \"-=\" in s:\n",
        "            s = s.split(\"-=\")[0]\n",
        "    s = s.strip()\n",
        "    if len(s) == 0:\n",
        "        return [\"nothing\"]\n",
        "    tokens = [t.text.replace(\"'\", \"\") for t in tokenizer(s)]\n",
        "    # NORMALIZE WORDS\n",
        "    #tokens = [t.norm_ for t in tokenizer(s)]\n",
        "    if lower_case:\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "    return tokens + [\"<|>\"]\n",
        "\n",
        "\n",
        "def max_len(list_of_list):\n",
        "    return max(map(len, list_of_list))\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, maxlen=None, dtype='int32', value=0.):\n",
        "    '''\n",
        "    Partially borrowed from Keras\n",
        "    # Arguments\n",
        "        sequences: list of lists where each element is a sequence\n",
        "        maxlen: int, maximum length\n",
        "        dtype: type to cast the resulting sequence.\n",
        "        value: float, value to pad the sequences to the desired value.\n",
        "    # Returns\n",
        "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
        "    '''\n",
        "    lengths = [len(s) for s in sequences]\n",
        "    nb_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if len(s) == 0:\n",
        "            continue  # empty list was found\n",
        "        # pre truncating\n",
        "        trunc = s[-maxlen:]\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "        # post padding\n",
        "        x[idx, :len(trunc)] = trunc\n",
        "    return x\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim, enable_cuda=False):\n",
        "    emb_df = pd.read_csv(path, sep=\" \", index_col=0, \n",
        "                         header=None, quoting=csv.QUOTE_NONE)\n",
        "    \n",
        "    embeddings = np.zeros((len(word2idx), embedding_dim))\n",
        "    for word, weights in emb_df.iterrows():\n",
        "        index = word2idx.get(word, None)\n",
        "        if index != None:\n",
        "            embeddings[index] = weights.values\n",
        "    \n",
        "    if enable_cuda:\n",
        "        return torch.from_numpy(embeddings).float().cuda()\n",
        "    return torch.from_numpy(embeddings).float()\n",
        "    \n",
        "def freeze_layer(layer):\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HTSoK93tM6V",
        "colab_type": "text"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2VSAkD3s7Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def masked_mean(x, m=None, dim=-1):\n",
        "    \"\"\"\n",
        "        mean pooling when there're paddings\n",
        "        input:  tensor: batch x time x h\n",
        "                mask:   batch x time\n",
        "        output: tensor: batch x h\n",
        "    \"\"\"\n",
        "    if m is None:\n",
        "        return torch.mean(x, dim=dim)\n",
        "    mask_sum = torch.sum(m, dim=-1)  # batch\n",
        "    res = torch.sum(x, dim=1)  # batch x h\n",
        "    res = res / (mask_sum.unsqueeze(-1) + 1e-6)\n",
        "    return res\n",
        "\n",
        "\n",
        "class Embedding(torch.nn.Module):\n",
        "    '''\n",
        "    inputs: x:          batch x seq (x is post-padded by 0s)\n",
        "    outputs:embedding:  batch x seq x emb\n",
        "            mask:       batch x seq\n",
        "    '''\n",
        "\n",
        "    def __init__(self, embedding_size, vocab_size, enable_cuda=False):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.enable_cuda = enable_cuda\n",
        "        self.embedding_layer = torch.nn.Embedding(self.vocab_size, self.embedding_size, padding_idx=0)\n",
        "        \n",
        "    def set_weights(self, weights):\n",
        "        self.embedding_layer.weight = torch.nn.Parameter(weights)\n",
        "\n",
        "    def compute_mask(self, x):\n",
        "        mask = torch.ne(x, 0).float()\n",
        "        if self.enable_cuda:\n",
        "            mask = mask.cuda()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.embedding_layer(x)  # batch x time x emb\n",
        "        mask = self.compute_mask(x)  # batch x time\n",
        "        return embeddings, mask\n",
        "        \n",
        "\n",
        "class FastUniLSTM(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Adapted from https://github.com/facebookresearch/DrQA/\n",
        "    now supports:   different rnn size for each layer\n",
        "                    all zero rows in batch (from time distributed layer, by reshaping certain dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ninp, nhids, bidir=False, dropout_between_rnn_layers=0.):\n",
        "        super(FastUniLSTM, self).__init__()\n",
        "        self.ninp = ninp\n",
        "        self.nhids = nhids\n",
        "        self.nlayers = len(self.nhids)\n",
        "        self.dropout_between_rnn_layers = dropout_between_rnn_layers\n",
        "        self.stack_rnns(bidir)\n",
        "\n",
        "    def stack_rnns(self, bidir):\n",
        "        rnns = [torch.nn.LSTM(self.ninp if i == 0 else self.nhids[i - 1],\n",
        "                              self.nhids[i],\n",
        "                              num_layers=1,\n",
        "                              bidirectional=bidir) for i in range(self.nlayers)]\n",
        "            \n",
        "        self.rnns = torch.nn.ModuleList(rnns)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        def pad_(tensor, n):\n",
        "            if n > 0:\n",
        "                zero_pad = torch.autograd.Variable(torch.zeros((n,) + tensor.size()[1:]))\n",
        "                if x.is_cuda:\n",
        "                    zero_pad = zero_pad.cuda()\n",
        "                tensor = torch.cat([tensor, zero_pad])\n",
        "            return tensor\n",
        "\n",
        "        \"\"\"\n",
        "        inputs: x:          batch x time x inp\n",
        "                mask:       batch x time\n",
        "        output: encoding:   batch x time x hidden[-1]\n",
        "        \"\"\"\n",
        "        # Compute sorted sequence lengths\n",
        "        batch_size = x.size(0)\n",
        "        lengths = mask.data.eq(1).long().sum(1)  # .squeeze()\n",
        "        _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
        "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
        "\n",
        "        lengths = list(lengths[idx_sort])\n",
        "        idx_sort = torch.autograd.Variable(idx_sort)\n",
        "        idx_unsort = torch.autograd.Variable(idx_unsort)\n",
        "\n",
        "        # Sort x\n",
        "        x = x.index_select(0, idx_sort)\n",
        "\n",
        "        # remove non-zero rows, and remember how many zeros\n",
        "        n_nonzero = np.count_nonzero(lengths)\n",
        "        n_zero = batch_size - n_nonzero\n",
        "        if n_zero != 0:\n",
        "            lengths = lengths[:n_nonzero]\n",
        "            x = x[:n_nonzero]\n",
        "\n",
        "        # Transpose batch and sequence dims\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        # Pack it up\n",
        "        rnn_input = torch.nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
        "\n",
        "        # Encode all layers\n",
        "        outputs = [rnn_input]\n",
        "        for i in range(self.nlayers):\n",
        "            rnn_input = outputs[-1]\n",
        "\n",
        "            # dropout between rnn layers\n",
        "            if self.dropout_between_rnn_layers > 0:\n",
        "                dropout_input = F.dropout(rnn_input.data,\n",
        "                                          p=self.dropout_between_rnn_layers,\n",
        "                                          training=self.training)\n",
        "                rnn_input = torch.nn.utils.rnn.PackedSequence(dropout_input,\n",
        "                                                              rnn_input.batch_sizes)\n",
        "            seq, last = self.rnns[i](rnn_input)\n",
        "            outputs.append(seq)\n",
        "            if i == self.nlayers - 1:\n",
        "                # last layer\n",
        "                last_state = last[0]  # (num_layers * num_directions, batch, hidden_size)\n",
        "                last_state = last_state[0]  # batch x hidden_size\n",
        "\n",
        "        # Unpack everything\n",
        "        for i, o in enumerate(outputs[1:], 1):\n",
        "            outputs[i] = torch.nn.utils.rnn.pad_packed_sequence(o)[0]\n",
        "        output = outputs[-1]\n",
        "\n",
        "        # Transpose and unsort\n",
        "        output = output.transpose(0, 1)  # batch x time x enc\n",
        "\n",
        "        # re-padding\n",
        "        output = pad_(output, n_zero)\n",
        "        last_state = pad_(last_state, n_zero)\n",
        "\n",
        "        output = output.index_select(0, idx_unsort)\n",
        "        last_state = last_state.index_select(0, idx_unsort)\n",
        "\n",
        "        # Pad up to original batch sequence length\n",
        "        if output.size(1) != mask.size(1):\n",
        "            padding = torch.zeros(output.size(0),\n",
        "                                  mask.size(1) - output.size(1),\n",
        "                                  output.size(2)).type(output.data.type())\n",
        "            output = torch.cat([output, torch.autograd.Variable(padding)], 1)\n",
        "\n",
        "        output = output.contiguous() * mask.unsqueeze(-1)\n",
        "        return output, last_state, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mq5lPlnA_Si",
        "colab_type": "text"
      },
      "source": [
        "## Noisy nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra-XRZ6SA_rX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoisyLinear(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features, std_init=0.4):\n",
        "        super(NoisyLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.std_init = std_init\n",
        "        self.weight_mu = torch.nn.Parameter(torch.empty(out_features, in_features))\n",
        "        self.weight_sigma = torch.nn.Parameter(torch.empty(out_features, in_features))\n",
        "        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n",
        "        self.bias_mu = torch.nn.Parameter(torch.empty(out_features))\n",
        "        self.bias_sigma = torch.nn.Parameter(torch.empty(out_features))\n",
        "        self.register_buffer('bias_epsilon', torch.empty(out_features))\n",
        "        self.reset_parameters()\n",
        "        self.sample_noise()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        mu_range = 1.0 / math.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
        "\n",
        "    def _scale_noise(self, size):\n",
        "        x = torch.randn(size)\n",
        "        return x.sign().mul_(x.abs().sqrt_())\n",
        "\n",
        "    def sample_noise(self):\n",
        "        epsilon_in = self._scale_noise(self.in_features)\n",
        "        epsilon_out = self._scale_noise(self.out_features)\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(epsilon_out)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if self.training:\n",
        "            return F.linear(inp, self.weight_mu + self.weight_sigma * self.weight_epsilon, self.bias_mu + self.bias_sigma * self.bias_epsilon)\n",
        "        else:\n",
        "            return F.linear(inp, self.weight_mu, self.bias_mu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0NRD6XDtRKa",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4txzFctSFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_DQN(torch.nn.Module):\n",
        "    model_name = 'lstm_dqn'\n",
        "\n",
        "    def __init__(self, model_config, embedding_weights, word_vocab, generate_length=5, enable_cuda=False):\n",
        "        super(LSTM_DQN, self).__init__()\n",
        "        self.model_config = model_config\n",
        "        self.enable_cuda = enable_cuda\n",
        "        self.word_vocab_size = len(word_vocab)\n",
        "        self.id2word = word_vocab\n",
        "        self.generate_length = generate_length\n",
        "        self.read_config()\n",
        "        self._def_layers(embedding_weights)\n",
        "        self.init_weights()\n",
        "        #self.print_parameters()\n",
        "\n",
        "    def print_parameters(self):\n",
        "        print(self)\n",
        "        amount = 0\n",
        "        for p in self.parameters():\n",
        "            amount += np.prod(p.size())\n",
        "        print(\"Total number of parameters: {}\".format(amount))\n",
        "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
        "        amount = 0\n",
        "        for p in parameters:\n",
        "            amount += np.prod(p.size())\n",
        "        print(\"Number of trainable parameters: {}\".format(amount))\n",
        "\n",
        "    def read_config(self):\n",
        "        # model config\n",
        "        self.freeze_embedding = self.model_config['freeze_embedding']\n",
        "        self.embedding_size = self.model_config['embedding_size']\n",
        "        self.encoder_rnn_hidden_size = self.model_config['encoder_rnn_hidden_size']\n",
        "        self.action_scorer_hidden_dim = self.model_config['action_scorer_hidden_dim']\n",
        "        self.dropout_between_rnn_layers = self.model_config['dropout_between_rnn_layers']\n",
        "        self.bidirectional_lstm = self.model_config['bidirectional_lstm']\n",
        "\n",
        "    def _def_layers(self, embedding_weights=None):\n",
        "        # word embeddings\n",
        "        self.word_embedding = Embedding(embedding_size=self.embedding_size,\n",
        "                                        vocab_size=self.word_vocab_size,\n",
        "                                        enable_cuda=self.enable_cuda)\n",
        "        if not(embedding_weights is None):\n",
        "            self.word_embedding.set_weights(embedding_weights)\n",
        "            print(\"Embedding imported!\")\n",
        "            if self.freeze_embedding:\n",
        "                freeze_layer(self.word_embedding.embedding_layer)\n",
        "                print(\"Embedding freezed!\")\n",
        "            \n",
        "        # lstm encoder\n",
        "        self.encoder = FastUniLSTM(ninp=self.embedding_size,\n",
        "                                   nhids=self.encoder_rnn_hidden_size,\n",
        "                                   bidir=self.bidirectional_lstm,\n",
        "                                   dropout_between_rnn_layers=self.dropout_between_rnn_layers)\n",
        "        \n",
        "        shared_input_size = self.encoder_rnn_hidden_size[-1]\n",
        "        shared_input_size *= 2 if self.bidirectional_lstm else 1\n",
        "        self.action_scorer_shared = torch.nn.Linear(shared_input_size, self.action_scorer_hidden_dim)\n",
        "\n",
        "        action_scorers = []\n",
        "        for _ in range(self.generate_length):\n",
        "            action_scorers.append(\n",
        "                NoisyLinear(self.action_scorer_hidden_dim, \n",
        "                            self.word_vocab_size, \n",
        "                            std_init=self.model_config['noisy_std']))\n",
        "        self.action_scorers = torch.nn.ModuleList(action_scorers)\n",
        "        self.fake_recurrent_mask = None\n",
        "\n",
        "    def init_weights(self):\n",
        "        torch.nn.init.xavier_uniform_(self.action_scorer_shared.weight.data)\n",
        "        \n",
        "        for i in range(len(self.action_scorers)):\n",
        "            self.action_scorers[i].sample_noise()\n",
        "        #    torch.nn.init.xavier_uniform_(self.action_scorers[i].weight.data)\n",
        "        self.action_scorer_shared.bias.data.fill_(0)\n",
        "\n",
        "    def representation_generator(self, _input_words):\n",
        "        embeddings, mask = self.word_embedding.forward(_input_words)  # batch x time x emb\n",
        "        encoding_sequence, _, _ = self.encoder.forward(embeddings, mask)  # batch x time x h\n",
        "        mean_encoding = masked_mean(encoding_sequence, mask)  # batch x h\n",
        "        return mean_encoding\n",
        "\n",
        "    def action_scorer(self, state_representation):\n",
        "        hidden = self.action_scorer_shared.forward(state_representation)  # batch x hid\n",
        "        hidden = F.relu(hidden)  # batch x hid\n",
        "        action_ranks = []\n",
        "        for i in range(len(self.action_scorers)):\n",
        "            action_ranks.append(self.action_scorers[i].forward(hidden))  # batch x n_vocab\n",
        "        return action_ranks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoRlncyh_U5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declare global embedding_weights\n",
        "embedding_weights = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vJ6KiC6nYA1",
        "colab_type": "text"
      },
      "source": [
        "Cache score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75-I610unU8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HistoryScoreCache(object):\n",
        "\n",
        "    def __init__(self, capacity=1):\n",
        "        self.capacity = capacity\n",
        "        self.reset()\n",
        "\n",
        "    def push(self, stuff):\n",
        "        \"\"\"stuff is float.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(stuff)\n",
        "        else:\n",
        "            self.memory = self.memory[1:] + [stuff]\n",
        "\n",
        "    def get_avg(self):\n",
        "        return np.mean(np.array(self.memory))\n",
        "\n",
        "    def reset(self):\n",
        "        self.memory = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F98HXiXKnjIH",
        "colab_type": "text"
      },
      "source": [
        "## Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJv3PcbJnXG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a snapshot of state to be stored in replay memory\n",
        "Transition = namedtuple('Transition', ('observation_id_list', 'word_indices',\n",
        "                                       'reward', 'mask', 'done',\n",
        "                                       'next_observation_id_list',\n",
        "                                       'next_word_masks'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XBFZOxHV6CD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrioritizedReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity=100000, priority_fraction=0.0):\n",
        "        # prioritized replay memory\n",
        "        self.priority_fraction = priority_fraction\n",
        "        self.alpha_capacity = int(capacity * priority_fraction)\n",
        "        self.beta_capacity = capacity - self.alpha_capacity\n",
        "        self.alpha_memory, self.beta_memory = [], []\n",
        "        self.alpha_position, self.beta_position = 0, 0\n",
        "\n",
        "    def push(self, is_prior, transition):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if self.priority_fraction == 0.0:\n",
        "            is_prior = False\n",
        "        if is_prior:\n",
        "            if len(self.alpha_memory) < self.alpha_capacity:\n",
        "                self.alpha_memory.append(None)\n",
        "            self.alpha_memory[self.alpha_position] = transition\n",
        "            self.alpha_position = (self.alpha_position + 1) % self.alpha_capacity\n",
        "        else:\n",
        "            if len(self.beta_memory) < self.beta_capacity:\n",
        "                self.beta_memory.append(None)\n",
        "            self.beta_memory[self.beta_position] = transition\n",
        "            self.beta_position = (self.beta_position + 1) % self.beta_capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if self.priority_fraction == 0.0 or len(self.alpha_memory) == 0:\n",
        "            from_beta = min(batch_size, len(self.beta_memory))\n",
        "            res = random.sample(self.beta_memory, from_beta)\n",
        "        elif len(self.beta_memory) == 0:\n",
        "            from_alpha = min(batch_size, len(self.alpha_memory))\n",
        "            res = random.sample(self.alpha_memory, from_alpha)\n",
        "        else:\n",
        "            priority_batch = int(self.priority_fraction * batch_size)\n",
        "            from_alpha = min(priority_batch, len(self.alpha_memory))\n",
        "            from_beta = min(batch_size - priority_batch, len(self.beta_memory))\n",
        "            res = random.sample(self.alpha_memory, from_alpha) + random.sample(self.beta_memory, from_beta)\n",
        "        random.shuffle(res)\n",
        "        return res\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.alpha_memory) + len(self.beta_memory)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuUH9-QrnQxI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qMQboCLcEu7",
        "colab_type": "text"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jF57tZRBzMa8",
        "colab": {}
      },
      "source": [
        "class CustomAgent:\n",
        "    def __init__(self):\n",
        "        global embedding_weights\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            word_vocab: List of words supported.\n",
        "        \"\"\"\n",
        "        self.mode = \"train\"\n",
        "        with open(\"./vocab.txt\") as f:\n",
        "            self.word_vocab = f.read().split(\"\\n\")\n",
        "        with open(\"config.yaml\") as reader:\n",
        "            self.config = yaml.safe_load(reader)\n",
        "        self.word2id = {}\n",
        "        for i, w in enumerate(self.word_vocab):\n",
        "            self.word2id[w] = i\n",
        "        self.EOS_id = self.word2id[\"</S>\"]\n",
        "\n",
        "        self.batch_size = self.config['training']['batch_size']\n",
        "        self.max_nb_steps_per_episode = self.config['training']['max_nb_steps_per_episode']\n",
        "        self.nb_epochs = self.config['training']['nb_epochs']\n",
        "        self.qval_noise_std = self.config['training']['qval_noise_std']\n",
        "\n",
        "        # Set the random seed manually for reproducibility.\n",
        "        np.random.seed(self.config['general']['random_seed'])\n",
        "        torch.manual_seed(self.config['general']['random_seed'])\n",
        "        if torch.cuda.is_available():\n",
        "            if not self.config['general']['use_cuda']:\n",
        "                logging.warning(\"WARNING: CUDA device detected but 'use_cuda: false' found in config.yaml\")\n",
        "                self.use_cuda = False\n",
        "            else:\n",
        "                torch.backends.cudnn.deterministic = True\n",
        "                torch.cuda.manual_seed(self.config['general']['random_seed'])\n",
        "                self.use_cuda = True\n",
        "        else:\n",
        "            self.use_cuda = False\n",
        "        \n",
        "        if embedding_weights is None:\n",
        "            print(\"Start loading glove\")\n",
        "            embedding_weights = load_glove_embeddings(\n",
        "                self.config[\"model\"]['embedding_path'],\n",
        "                self.word2id,\n",
        "                embedding_dim=self.config[\"model\"]['embedding_size'],\n",
        "                enable_cuda=self.use_cuda\n",
        "            )\n",
        "        print(\"Creating Q-Network\")\n",
        "        embedding_weights1 = embedding_weights.clone().detach() if not(embedding_weights is None) else None\n",
        "        self.model = LSTM_DQN(model_config=self.config[\"model\"],\n",
        "                              embedding_weights=embedding_weights1,\n",
        "                              word_vocab=self.word_vocab,\n",
        "                              enable_cuda=self.use_cuda)\n",
        "        print(\"Creating Target Network\")\n",
        "        embedding_weights2 = embedding_weights.clone().detach() if not(embedding_weights is None) else None\n",
        "        self.target_model = LSTM_DQN(model_config=self.config[\"model\"],\n",
        "                                     embedding_weights=embedding_weights2,\n",
        "                                     word_vocab=self.word_vocab,\n",
        "                                     enable_cuda=self.use_cuda)\n",
        "        \n",
        "        self.target_model.print_parameters()\n",
        "        \n",
        "        self.update_target_model_count = 0\n",
        "        self.target_model_update_frequency = self.config['training']['target_model_update_frequency']\n",
        "\n",
        "        self.experiment_tag = self.config['checkpoint']['experiment_tag']\n",
        "        self.model_checkpoint_dir = self.config['checkpoint']['model_checkpoint_dir']\n",
        "        self.save_frequency = self.config['checkpoint']['save_frequency']\n",
        "\n",
        "        if self.config['checkpoint']['load_pretrained']:\n",
        "            self.load_pretrained_model(self.model_checkpoint_dir)\n",
        "        if self.use_cuda:\n",
        "            self.model.cuda()\n",
        "            self.target_model.cuda()\n",
        "\n",
        "        self.replay_batch_size = self.config['general']['replay_batch_size']\n",
        "        self.replay_memory = PrioritizedReplayMemory(self.config['general']['replay_memory_capacity'],\n",
        "                                                     priority_fraction=self.config['general']['replay_memory_priority_fraction'])\n",
        "\n",
        "        # optimizer\n",
        "        parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "        self.optimizer = torch.optim.Adam(parameters, lr=self.config['training']['optimizer']['learning_rate'])\n",
        "        \n",
        "        # n-step\n",
        "        self.nsteps = self.config['general']['nsteps']\n",
        "        self.nstep_buffer = []\n",
        "\n",
        "        # epsilon greedy\n",
        "        self.epsilon_anneal_episodes = self.config['general']['epsilon_anneal_episodes']\n",
        "        self.epsilon_anneal_from = self.config['general']['epsilon_anneal_from']\n",
        "        self.epsilon_anneal_to = self.config['general']['epsilon_anneal_to']\n",
        "        self.epsilon = self.epsilon_anneal_from\n",
        "        self.update_per_k_game_steps = self.config['general']['update_per_k_game_steps']\n",
        "        self.clip_grad_norm = self.config['training']['optimizer']['clip_grad_norm']\n",
        "\n",
        "        self.nlp = spacy.load('en', disable=['ner', 'parser', 'tagger'])\n",
        "        self.preposition_map = {\"take\": \"from\",\n",
        "                                \"chop\": \"with\",\n",
        "                                \"slice\": \"with\",\n",
        "                                \"dice\": \"with\",\n",
        "                                \"cook\": \"with\",\n",
        "                                \"insert\": \"into\",\n",
        "                                \"put\": \"on\"}\n",
        "        self.single_word_verbs = set([\"inventory\", \"look\"])\n",
        "        self.discount_gamma = self.config['general']['discount_gamma']\n",
        "        self.current_episode = 0\n",
        "        self.current_step = 0\n",
        "        self._epsiode_has_started = False\n",
        "        self.history_avg_scores = HistoryScoreCache(capacity=1000)\n",
        "        self.best_avg_score_so_far = 0.0\n",
        "        self.loss = []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Tell the agent that it's training phase.\n",
        "        \"\"\"\n",
        "        self.mode = \"train\"\n",
        "        self.model.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        Tell the agent that it's evaluation phase.\n",
        "        \"\"\"\n",
        "        self.mode = \"eval\"\n",
        "        self.model.eval()\n",
        "\n",
        "    def _start_episode(self, obs: List[str], infos: Dict[str, List[Any]]) -> None:\n",
        "        \"\"\"\n",
        "        Prepare the agent for the upcoming episode.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Initial feedback for each game.\n",
        "            infos: Additional information for each game.\n",
        "        \"\"\"\n",
        "        self.init(obs, infos)\n",
        "        self._epsiode_has_started = True\n",
        "\n",
        "    def _end_episode(self, obs: List[str], scores: List[int], infos: Dict[str, List[Any]]) -> None:\n",
        "        \"\"\"\n",
        "        Tell the agent the episode has terminated.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            score: The score obtained so far for each game.\n",
        "            infos: Additional information for each game.\n",
        "        \"\"\"\n",
        "        self.finish()\n",
        "        self._epsiode_has_started = False\n",
        "\n",
        "    def load_pretrained_model(self, load_from_dir):\n",
        "        \"\"\"\n",
        "        Load the pretrained model's last checkpoint from a directory\n",
        "\n",
        "        Arguments:\n",
        "            load_from_dir: Directory with save model parameters\n",
        "        \"\"\"\n",
        "        \n",
        "        checkpoints_glob = glob.glob(os.path.join(load_from_dir, '*.pt'))\n",
        "#         reg = re.compile('.*{}_episode_(\\d+)\\.pt'.format(self.experiment_tag))\n",
        "#         print(\"\\nREG: {}\".format(reg))\n",
        "#         arg_max = np.argmax([ int(re.sub(reg, r'\\1', i)) for i in checkpoints_glob ])\n",
        "#         load_from = checkpoints_glob[arg_max]\n",
        "#         if len(checkpoints_glob) == 0:\n",
        "#             print(\"No model checkpoints to load from: \" + load_from_dir)\n",
        "#             return\n",
        "        \n",
        "#         arg_max = np.argmax([ int(re.sub(r'.*_(\\d+)\\.pt', r'\\1', i)) for i in checkpoints_glob ])\n",
        "#         load_from = checkpoints_glob[arg_max]\n",
        "#         print(\"loading model from {}\\n\".format(load_from))\n",
        "        load_from = os.path.join(load_from_dir, 'lstm-ddqn-noisy_episode_350.pt')\n",
        "        print(\"LOADING FROM: {}\".format(load_from) )\n",
        "        \n",
        "        try:\n",
        "            if self.use_cuda:\n",
        "                state_dict = torch.load(load_from)\n",
        "            else:\n",
        "                state_dict = torch.load(load_from, map_location='cpu')\n",
        "            self.model.load_state_dict(state_dict)\n",
        "        except:\n",
        "            print(\"Failed to load model checkpoint...\")\n",
        "        \n",
        "        memory_files = glob.glob(os.path.join(load_from_dir, '*.pickle'))\n",
        "        if len(memory_files) == 0:\n",
        "            print(\"No replay memory saves to load from: \" + load_from_dir)\n",
        "            return\n",
        "        print(\"loading memory from: {}\".format(memory_files[0]))\n",
        "        \n",
        "        try:\n",
        "            with open(memory_files[0], 'rb') as f:\n",
        "                self.replay_memory = pickle.load(f)\n",
        "        except:\n",
        "            print(\"Failed to load replay memory checkpoint...\")\n",
        "        \n",
        "        \n",
        "\n",
        "    def select_additional_infos(self) -> EnvInfos:\n",
        "        \"\"\"\n",
        "        Returns what additional information should be made available at each game step.\n",
        "\n",
        "        Requested information will be included within the `infos` dictionary\n",
        "        passed to `CustomAgent.act()`. To request specific information, create a\n",
        "        :py:class:`textworld.EnvInfos <textworld.envs.wrappers.filter.EnvInfos>`\n",
        "        and set the appropriate attributes to `True`. The possible choices are:\n",
        "\n",
        "        * `description`: text description of the current room, i.e. output of the `look` command;\n",
        "        * `inventory`: text listing of the player's inventory, i.e. output of the `inventory` command;\n",
        "        * `max_score`: maximum reachable score of the game;\n",
        "        * `objective`: objective of the game described in text;\n",
        "        * `entities`: names of all entities in the game;\n",
        "        * `verbs`: verbs understood by the the game;\n",
        "        * `command_templates`: templates for commands understood by the the game;\n",
        "        * `admissible_commands`: all commands relevant to the current state;\n",
        "\n",
        "        In addition to the standard information, game specific information\n",
        "        can be requested by appending corresponding strings to the `extras`\n",
        "        attribute. For this competition, the possible extras are:\n",
        "\n",
        "        * `'recipe'`: description of the cookbook;\n",
        "        * `'walkthrough'`: one possible solution to the game (not guaranteed to be optimal);\n",
        "\n",
        "        Example:\n",
        "            Here is an example of how to request information and retrieve it.\n",
        "\n",
        "            >>> from textworld import EnvInfos\n",
        "            >>> request_infos = EnvInfos(description=True, inventory=True, extras=[\"recipe\"])\n",
        "            ...\n",
        "            >>> env = gym.make(env_id)\n",
        "            >>> ob, infos = env.reset()\n",
        "            >>> print(infos[\"description\"])\n",
        "            >>> print(infos[\"inventory\"])\n",
        "            >>> print(infos[\"extra.recipe\"])\n",
        "\n",
        "        Notes:\n",
        "            The following information *won't* be available at test time:\n",
        "\n",
        "            * 'walkthrough'\n",
        "        \"\"\"\n",
        "        request_infos = EnvInfos()\n",
        "        request_infos.description = True\n",
        "        request_infos.inventory = True\n",
        "        request_infos.entities = True\n",
        "        request_infos.verbs = True\n",
        "        request_infos.max_score = True\n",
        "        request_infos.has_won = True\n",
        "        request_infos.has_lost = True\n",
        "        request_infos.extras = [\"recipe\"]\n",
        "        return request_infos\n",
        "\n",
        "    def init(self, obs: List[str], infos: Dict[str, List[Any]]):\n",
        "        \"\"\"\n",
        "        Prepare the agent for the upcoming games.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            infos: Additional information for each game.\n",
        "        \"\"\"\n",
        "        # reset agent, get vocabulary masks for verbs / adjectives / nouns\n",
        "        self.scores = []\n",
        "        self.dones = []\n",
        "        self.prev_actions = [\"\" for _ in range(len(obs))]\n",
        "        # get word masks\n",
        "        batch_size = len(infos[\"verbs\"])\n",
        "        verbs_word_list = infos[\"verbs\"]\n",
        "        noun_word_list, adj_word_list = [], []\n",
        "        for entities in infos[\"entities\"]:\n",
        "            tmp_nouns, tmp_adjs = [], []\n",
        "            for name in entities:\n",
        "                split = name.split()\n",
        "                tmp_nouns.append(split[-1])\n",
        "                if len(split) > 1:\n",
        "                    tmp_adjs += split[:-1]\n",
        "            noun_word_list.append(list(set(tmp_nouns)))\n",
        "            adj_word_list.append(list(set(tmp_adjs)))\n",
        "\n",
        "        verb_mask = np.zeros((batch_size, len(self.word_vocab)), dtype=\"float32\")\n",
        "        noun_mask = np.zeros((batch_size, len(self.word_vocab)), dtype=\"float32\")\n",
        "        adj_mask = np.zeros((batch_size, len(self.word_vocab)), dtype=\"float32\")\n",
        "        for i in range(batch_size):\n",
        "            for w in verbs_word_list[i]:\n",
        "                if w in self.word2id:\n",
        "                    verb_mask[i][self.word2id[w]] = 1.0\n",
        "            for w in noun_word_list[i]:\n",
        "                if w in self.word2id:\n",
        "                    noun_mask[i][self.word2id[w]] = 1.0\n",
        "            for w in adj_word_list[i]:\n",
        "                if w in self.word2id:\n",
        "                    adj_mask[i][self.word2id[w]] = 1.0\n",
        "        second_noun_mask = copy.copy(noun_mask)\n",
        "        second_adj_mask = copy.copy(adj_mask)\n",
        "        second_noun_mask[:, self.EOS_id] = 1.0\n",
        "        adj_mask[:, self.EOS_id] = 1.0\n",
        "        second_adj_mask[:, self.EOS_id] = 1.0\n",
        "        self.word_masks_np = [verb_mask, adj_mask, noun_mask, second_adj_mask, second_noun_mask]\n",
        "\n",
        "        self.cache_description_id_list = None\n",
        "        self.cache_chosen_indices = None\n",
        "        self.current_step = 0\n",
        "        \n",
        "    def append_to_replay(self, is_prior, transition):\n",
        "        self.nstep_buffer.append((is_prior, transition))\n",
        "\n",
        "        if len(self.nstep_buffer) < self.nsteps:\n",
        "            return\n",
        "        \n",
        "        R = sum([self.nstep_buffer[i][1].reward * (self.discount_gamma**i) for i in range(self.nsteps)])\n",
        "        prior, transition = self.nstep_buffer.pop(0)\n",
        "\n",
        "        self.replay_memory.push(prior, transition._replace(reward=R))\n",
        "\n",
        "\n",
        "    def get_game_step_info(self, obs: List[str], infos: Dict[str, List[Any]]):\n",
        "        \"\"\"\n",
        "        Get all the available information, and concat them together to be tensor for\n",
        "        a neural model. we use post padding here, all information are tokenized here.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            infos: Additional information for each game.\n",
        "        \"\"\"\n",
        "\n",
        "        inventory_token_list = [preproc(item, tokenizer=self.nlp) for item in infos[\"inventory\"]]\n",
        "        inventory_id_list = [_words_to_ids(tokens, self.word2id) for tokens in inventory_token_list]\n",
        "        #print(\"Inventory: \\n{}\\n\".format(inventory_token_list))\n",
        "\n",
        "        feedback_token_list = [preproc(item, str_type='feedback', tokenizer=self.nlp) for item in obs]\n",
        "        feedback_id_list = [_words_to_ids(tokens, self.word2id) for tokens in feedback_token_list]\n",
        "        #print(\"Feedback: \\n{}\\n\".format(feedback_token_list))\n",
        "\n",
        "        quest_token_list = [preproc(item, tokenizer=self.nlp) for item in infos[\"extra.recipe\"]]\n",
        "        quest_id_list = [_words_to_ids(tokens, self.word2id) for tokens in quest_token_list]\n",
        "        #print(\"Quest:\\n{}\\n\".format(quest_token_list))\n",
        "\n",
        "        prev_action_token_list = [preproc(item, tokenizer=self.nlp) for item in self.prev_actions]\n",
        "        prev_action_id_list = [_words_to_ids(tokens, self.word2id) for tokens in prev_action_token_list]\n",
        "        #print(\"Prev action:\\n{}\\n\".format(prev_action_token_list))\n",
        "        \n",
        "        description_token_list = [preproc(item, tokenizer=self.nlp) for item in infos[\"description\"]]\n",
        "        for i, d in enumerate(description_token_list):\n",
        "            if len(d) == 0:\n",
        "                description_token_list[i] = [\"end\"]  # if empty description, insert word \"end\"\n",
        "        \n",
        "        description_id_list = [_words_to_ids(tokens, self.word2id) for tokens in description_token_list]\n",
        "        description_id_list = [\n",
        "            _d +  _i + _q + _f + _pa \n",
        "            for (_d, _i, _q, _f, _pa) \n",
        "            in zip(description_id_list, inventory_id_list, quest_id_list, feedback_id_list, prev_action_id_list)\n",
        "        ]\n",
        "        \n",
        "        input_description = pad_sequences(description_id_list, maxlen=max_len(description_id_list)).astype('int32')\n",
        "        input_description = to_pt(input_description, self.use_cuda)\n",
        "\n",
        "        return input_description, description_id_list\n",
        "\n",
        "    def word_ids_to_commands(self, verb, adj, noun, adj_2, noun_2):\n",
        "        \"\"\"\n",
        "        Turn the 5 indices into actual command strings.\n",
        "\n",
        "        Arguments:\n",
        "            verb: Index of the guessing verb in vocabulary\n",
        "            adj: Index of the guessing adjective in vocabulary\n",
        "            noun: Index of the guessing noun in vocabulary\n",
        "            adj_2: Index of the second guessing adjective in vocabulary\n",
        "            noun_2: Index of the second guessing noun in vocabulary\n",
        "        \"\"\"\n",
        "        # turns 5 indices into actual command strings\n",
        "        if self.word_vocab[verb] in self.single_word_verbs:\n",
        "            return self.word_vocab[verb]\n",
        "        if adj == self.EOS_id:\n",
        "            res = self.word_vocab[verb] + \" \" + self.word_vocab[noun]\n",
        "        else:\n",
        "            res = self.word_vocab[verb] + \" \" + self.word_vocab[adj] + \" \" + self.word_vocab[noun]\n",
        "        if self.word_vocab[verb] not in self.preposition_map:\n",
        "            return res\n",
        "        if noun_2 == self.EOS_id:\n",
        "            return res\n",
        "        prep = self.preposition_map[self.word_vocab[verb]]\n",
        "        if adj_2 == self.EOS_id:\n",
        "            res = res + \" \" + prep + \" \" + self.word_vocab[noun_2]\n",
        "        else:\n",
        "            res =  res + \" \" + prep + \" \" + self.word_vocab[adj_2] + \" \" + self.word_vocab[noun_2]\n",
        "        return res\n",
        "\n",
        "    def get_chosen_strings(self, chosen_indices):\n",
        "        \"\"\"\n",
        "        Turns list of word indices into actual command strings.\n",
        "\n",
        "        Arguments:\n",
        "            chosen_indices: Word indices chosen by model.\n",
        "        \"\"\"\n",
        "        chosen_indices_np = [to_np(item)[:, 0] for item in chosen_indices]\n",
        "        res_str = []\n",
        "        batch_size = chosen_indices_np[0].shape[0]\n",
        "        for i in range(batch_size):\n",
        "            verb, adj, noun, adj_2, noun_2 = chosen_indices_np[0][i],\\\n",
        "                                             chosen_indices_np[1][i],\\\n",
        "                                             chosen_indices_np[2][i],\\\n",
        "                                             chosen_indices_np[3][i],\\\n",
        "                                             chosen_indices_np[4][i]\n",
        "            res_str.append(self.word_ids_to_commands(verb, adj, noun, adj_2, noun_2))\n",
        "        return res_str\n",
        "\n",
        "#     def choose_random_command(self, word_ranks, word_masks_np):\n",
        "#         \"\"\"\n",
        "#         Generate a command randomly, for epsilon greedy.\n",
        "\n",
        "#         Arguments:\n",
        "#             word_ranks: Q values for each word by model.action_scorer.\n",
        "#             word_masks_np: Vocabulary masks for words depending on their type (verb, adj, noun).\n",
        "#         \"\"\"\n",
        "#         batch_size = word_ranks[0].size(0)\n",
        "#         word_ranks_np = [to_np(item) for item in word_ranks]  # list of batch x n_vocab\n",
        "#         word_ranks_np = [r * m for r, m in zip(word_ranks_np, word_masks_np)]  # list of batch x n_vocab\n",
        "#         word_indices = []\n",
        "#         for i in range(len(word_ranks_np)):\n",
        "#             indices = []\n",
        "#             for j in range(batch_size):\n",
        "#                 msk = word_masks_np[i][j]  # vocab\n",
        "#                 indices.append(np.random.choice(len(msk), p=msk / np.sum(msk, -1)))\n",
        "#             word_indices.append(np.array(indices))\n",
        "#         #print(\"RANDOM: {}\".format(np.array(word_indices)))\n",
        "#         # word_indices: list of batch\n",
        "#         word_qvalues = [[] for _ in word_masks_np]\n",
        "#         for i in range(batch_size):\n",
        "#             for j in range(len(word_qvalues)):\n",
        "#                 word_qvalues[j].append(word_ranks[j][i][word_indices[j][i]])\n",
        "#         word_qvalues = [torch.stack(item) for item in word_qvalues]\n",
        "#         word_indices = [to_pt(item, self.use_cuda) for item in word_indices]\n",
        "#         word_indices = [item.unsqueeze(-1) for item in word_indices]  # list of batch x 1\n",
        "#         return word_qvalues, word_indices\n",
        "\n",
        "    def choose_random_command(self, word_ranks, word_masks_np):\n",
        "        batch_size = word_ranks[0].size(0)\n",
        "        word_ranks_np = [to_np(item) for item in word_ranks]  # list of batch x n_vocab\n",
        "        word_ranks_np = [r - np.min(r) for r in word_ranks_np] # minus the min value, so that all values are non-negative\n",
        "        random_ranks = np.random.normal(0, self.qval_noise_std, word_ranks_np[0].shape) \n",
        "        word_ranks_np = [r + random_ranks for r in word_ranks_np] # add noise      \n",
        "        word_ranks_np = [r * m for r, m in zip(word_ranks_np, word_masks_np)]  # list of batch x n_vocab\n",
        "        word_indices = [np.argmax(item, -1) for item in word_ranks_np]  # list of batch\n",
        "        word_qvalues = [[] for _ in word_masks_np]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            for j in range(len(word_qvalues)):\n",
        "                word_qvalues[j].append(word_ranks[j][i][word_indices[j][i]])\n",
        "\n",
        "        word_qvalues = [torch.stack(item) for item in word_qvalues]\n",
        "        word_indices = [to_pt(item, self.use_cuda) for item in word_indices]\n",
        "        word_indices = [item.unsqueeze(-1) for item in word_indices]  # list of batch x 1\n",
        "        return word_qvalues, word_indices\n",
        "\n",
        "    def choose_maxQ_command(self, word_ranks, word_masks_np):\n",
        "        \"\"\"\n",
        "        Generate a command by maximum q values, for epsilon greedy.\n",
        "\n",
        "        Arguments:\n",
        "            word_ranks: Q values for each word by model.action_scorer.\n",
        "            word_masks_np: Vocabulary masks for words depending on their type (verb, adj, noun).\n",
        "        \"\"\"\n",
        "        batch_size = word_ranks[0].size(0)\n",
        "        word_ranks_np = [to_np(item) for item in word_ranks]  # list of batch x n_vocab\n",
        "        word_ranks_np = [r - np.min(r) for r in word_ranks_np] # minus the min value, so that all values are non-negative\n",
        "        word_ranks_np = [r * m for r, m in zip(word_ranks_np, word_masks_np)]  # list of batch x n_vocab\n",
        "        word_indices = [np.argmax(item, -1) for item in word_ranks_np]  # list of batch\n",
        "        word_qvalues = [[] for _ in word_masks_np]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            for j in range(len(word_qvalues)):\n",
        "                word_qvalues[j].append(word_ranks[j][i][word_indices[j][i]])\n",
        "\n",
        "        word_qvalues = [torch.stack(item) for item in word_qvalues]\n",
        "        word_indices = [to_pt(item, self.use_cuda) for item in word_indices]\n",
        "        word_indices = [item.unsqueeze(-1) for item in word_indices]  # list of batch x 1\n",
        "        return word_qvalues, word_indices\n",
        "\n",
        "    def get_ranks(self, model, input_description):\n",
        "        \"\"\"\n",
        "        Given input description tensor, call model forward, to get Q values of words.\n",
        "\n",
        "        Arguments:\n",
        "            input_description: Input tensors, which include all the information chosen in\n",
        "            select_additional_infos() concatenated together.\n",
        "        \"\"\"\n",
        "        \n",
        "        state_representation = model.representation_generator(input_description)\n",
        "        #print(\"Size: {}\".format(state_representation.size()))\n",
        "        word_ranks = model.action_scorer(state_representation)  # each element in list has batch x n_vocab size\n",
        "        return word_ranks\n",
        "    \n",
        "    def act_eval(self, obs: List[str], scores: List[int], dones: List[bool], infos: Dict[str, List[Any]]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Acts upon the current list of observations, during evaluation.\n",
        "\n",
        "        One text command must be returned for each observation.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            score: The score obtained so far for each game (at previous step).\n",
        "            done: Whether a game is finished (at previous step).\n",
        "            infos: Additional information for each game.\n",
        "\n",
        "        Returns:\n",
        "            Text commands to be performed (one per observation).\n",
        "\n",
        "        Notes:\n",
        "            Commands returned for games marked as `done` have no effect.\n",
        "            The states for finished games are simply copy over until all\n",
        "            games are done, in which case `CustomAgent.finish()` is called\n",
        "            instead.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.current_step > 0:\n",
        "            # append scores / dones from previous step into memory\n",
        "            self.scores.append(scores)\n",
        "            self.dones.append(dones)\n",
        "\n",
        "        if all(dones):\n",
        "            self._end_episode(obs, scores, infos)\n",
        "            return  # Nothing to return.\n",
        "\n",
        "        input_description, _ = self.get_game_step_info(obs, infos)\n",
        "        word_ranks = self.get_ranks(self.model, input_description)  # list of batch x vocab\n",
        "        _, word_indices_maxq = self.choose_maxQ_command(word_ranks, self.word_masks_np)\n",
        "\n",
        "        chosen_indices = word_indices_maxq\n",
        "        chosen_indices = [item.detach() for item in chosen_indices]\n",
        "        chosen_strings = self.get_chosen_strings(chosen_indices)\n",
        "        self.prev_actions = chosen_strings\n",
        "        self.current_step += 1\n",
        "\n",
        "        return chosen_strings\n",
        "\n",
        "    def act(self, obs: List[str], scores: List[int], dones: List[bool], infos: Dict[str, List[Any]]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Acts upon the current list of observations.\n",
        "\n",
        "        One text command must be returned for each observation.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            score: The score obtained so far for each game (at previous step).\n",
        "            done: Whether a game is finished (at previous step).\n",
        "            infos: Additional information for each game.\n",
        "\n",
        "        Returns:\n",
        "            Text commands to be performed (one per observation).\n",
        "\n",
        "        Notes:\n",
        "            Commands returned for games marked as `done` have no effect.\n",
        "            The states for finished games are simply copy over until all\n",
        "            games are done, in which case `CustomAgent.finish()` is called\n",
        "            instead.\n",
        "        \"\"\"\n",
        "        if not self._epsiode_has_started:\n",
        "            self._start_episode(obs, infos)\n",
        "\n",
        "        if self.mode == \"eval\":\n",
        "            return self.act_eval(obs, scores, dones, infos)\n",
        "\n",
        "        if self.current_step > 0:\n",
        "            # append scores / dones from previous step into memory\n",
        "            self.scores.append(scores)\n",
        "            self.dones.append(dones)\n",
        "            # compute previous step's rewards and masks\n",
        "            rewards_np, rewards, mask_np, mask = self.compute_reward()\n",
        "        \n",
        "        # Sample for noisy nets\n",
        "        for i in range(len(self.model.action_scorers)):\n",
        "            self.model.action_scorers[i].sample_noise()\n",
        "\n",
        "        input_description, description_id_list = self.get_game_step_info(obs, infos)\n",
        "        # generate commands for one game step, epsilon greedy is applied, i.e.,\n",
        "        # there is epsilon of chance to generate random commands\n",
        "        \n",
        "        word_ranks = self.get_ranks(self.model, input_description)  # list of batch x vocab\n",
        "        \n",
        "        _, word_indices_maxq = self.choose_maxQ_command(word_ranks, self.word_masks_np)\n",
        "        _, word_indices_random = self.choose_random_command(word_ranks, self.word_masks_np)\n",
        "        \n",
        "        # random number for epsilon greedy\n",
        "        rand_num = np.random.uniform(low=0.0, high=1.0, size=(input_description.size(0), 1))\n",
        "        less_than_epsilon = (rand_num < self.epsilon).astype(\"float32\")  # batch\n",
        "        greater_than_epsilon = 1.0 - less_than_epsilon\n",
        "        less_than_epsilon = to_pt(less_than_epsilon, self.use_cuda, type='float')\n",
        "        greater_than_epsilon = to_pt(greater_than_epsilon, self.use_cuda, type='float')\n",
        "        less_than_epsilon, greater_than_epsilon = less_than_epsilon.long(), greater_than_epsilon.long()\n",
        "\n",
        "        chosen_indices = [\n",
        "            less_than_epsilon * idx_random + greater_than_epsilon * idx_maxq \n",
        "            for idx_random, idx_maxq in zip(word_indices_random, word_indices_maxq)\n",
        "        ]\n",
        "        chosen_indices = [item.detach() for item in chosen_indices]\n",
        "        chosen_strings = self.get_chosen_strings(chosen_indices)\n",
        "        random_idx = to_np(less_than_epsilon).astype('bool')\n",
        "        random_idx = random_idx.reshape((random_idx.shape[0]))\n",
        "        #print(\"\\nRAND IDX: {}\".format(random_idx))\n",
        "        #print(\"\\nMax commands: {}\".format(np.array(chosen_strings)[~random_idx]))\n",
        "        random_commands = np.array(chosen_strings)[random_idx]\n",
        "        #print(\"\\nRandom commands: {}\".format(random_commands))\n",
        "        self.prev_actions = chosen_strings\n",
        "\n",
        "        # push info from previous game step into replay memory\n",
        "        if self.current_step > 0:\n",
        "            for b in range(len(obs)):\n",
        "                if mask_np[b] == 0:\n",
        "                    continue\n",
        "                is_prior = rewards_np[b] > 0.0\n",
        "                t = Transition(self.cache_description_id_list[b], \n",
        "                               [ item[b] for item in self.cache_chosen_indices], \n",
        "                               rewards[b], \n",
        "                               mask[b], \n",
        "                               dones[b], \n",
        "                               description_id_list[b], \n",
        "                               [item[b] for item in self.word_masks_np])\n",
        "               # print(\"ACT: {}\".format(t.observation_id_list))\n",
        "                self.append_to_replay(is_prior, t)\n",
        "\n",
        "        # cache new info in current game step into caches\n",
        "        self.cache_description_id_list = description_id_list\n",
        "        self.cache_chosen_indices = chosen_indices\n",
        "\n",
        "        # update neural model by replaying snapshots in replay memory\n",
        "        if self.current_step > 0 and self.current_step % self.update_per_k_game_steps == 0:\n",
        "            loss = self.update()\n",
        "            #print(loss)\n",
        "            if loss is not None:\n",
        "                self.loss.append(to_np(loss).mean())\n",
        "                # Backpropagate\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward(retain_graph=True)\n",
        "                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.encoder.parameters(), self.clip_grad_norm)\n",
        "                self.optimizer.step()  # apply gradients\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        if all(dones):\n",
        "            self._end_episode(obs, scores, infos)\n",
        "            return  # Nothing to return.\n",
        "        return chosen_strings\n",
        "\n",
        "    def compute_reward(self):\n",
        "        \"\"\"\n",
        "        Compute rewards by agent. Note this is different from what the training/evaluation\n",
        "        scripts do. Agent keeps track of scores and other game information for training purpose.\n",
        "        \"\"\"\n",
        "        # mask = 1 if game is not finished or just finished at current step\n",
        "        if len(self.dones) == 1:\n",
        "            # it's not possible to finish a game at 0th step\n",
        "            mask = [1.0 for _ in self.dones[-1]]\n",
        "        else:\n",
        "            assert len(self.dones) > 1\n",
        "            mask = [1.0 if not self.dones[-2][i] else 0.0 for i in range(len(self.dones[-1]))]\n",
        "        mask = np.array(mask, dtype='float32')\n",
        "        mask_pt = to_pt(mask, self.use_cuda, type='float')\n",
        "        # rewards returned by game engine are always accumulated value the\n",
        "        # agent have recieved. so the reward it gets in the current game step\n",
        "        # is the new value minus values at previous step.\n",
        "        rewards = np.array(self.scores[-1], dtype='float32')  # batch\n",
        "        if len(self.scores) > 1:\n",
        "            prev_rewards = np.array(self.scores[-2], dtype='float32')\n",
        "            rewards = rewards - prev_rewards\n",
        "        rewards_pt = to_pt(rewards, self.use_cuda, type='float')\n",
        "\n",
        "        return rewards, rewards_pt, mask, mask_pt\n",
        "    \n",
        "    def update_target_model(self):\n",
        "        self.update_target_model_count = (self.update_target_model_count + 1) % self.target_model_update_frequency\n",
        "        if self.update_target_model_count == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        Update neural model in agent. In this example we follow algorithm\n",
        "        of updating model in dqn with replay memory.\n",
        "        \"\"\"\n",
        "        if len(self.replay_memory) < self.replay_batch_size:\n",
        "            return None\n",
        "        \n",
        "        self.update_target_model()\n",
        "        \n",
        "        #print(\"UPDATE! \\n Memory alpha size: {} | beta size: {}\\n\".format(len(self.replay_memory.alpha_memory), len(self.replay_memory.beta_memory)))\n",
        "        transitions = self.replay_memory.sample(self.replay_batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        observation_id_list = pad_sequences(batch.observation_id_list, maxlen=max_len(batch.observation_id_list)).astype('int32')\n",
        "        input_observation = to_pt(observation_id_list, self.use_cuda)\n",
        "        next_observation_id_list = pad_sequences(batch.next_observation_id_list, maxlen=max_len(batch.next_observation_id_list)).astype('int32')\n",
        "        next_input_observation = to_pt(next_observation_id_list, self.use_cuda)\n",
        "        chosen_indices = list(list(zip(*batch.word_indices)))\n",
        "        chosen_indices = [torch.stack(item, 0) for item in chosen_indices]  # list of batch x 1\n",
        "        \n",
        "        word_ranks = self.get_ranks(self.model, input_observation)  # list of batch x vocab\n",
        "        word_qvalues = [w_rank.gather(1, idx).squeeze(-1) for w_rank, idx in zip(word_ranks, chosen_indices)]  # list of batch\n",
        "        q_value = torch.mean(torch.stack(word_qvalues, -1), -1)  # batch\n",
        "\n",
        "        # Action selection, using q-network\n",
        "        next_word_ranks = self.get_ranks(self.target_model, next_input_observation) # batch x n_verb, batch x n_noun, batch x n_second_noun\n",
        "        next_word_masks = list(list(zip(*batch.next_word_masks)))\n",
        "        next_word_masks = [np.stack(item, 0) for item in next_word_masks]\n",
        "\n",
        "       # _, next_word_indexes = self.choose_maxQ_command(next_word_ranks, next_word_masks)\n",
        "        \n",
        "        # Action evaluation, using target network\n",
        "       # eval_next_word_ranks = self.get_ranks(self.target_model, next_input_observation)\n",
        "       # next_word_qvalues = [rank.gather(1, idx.detach()).squeeze(-1) for rank, idx in zip(eval_next_word_ranks, next_word_indexes)]\n",
        "        next_word_qvalues, _ = self.choose_maxQ_command(next_word_ranks, next_word_masks)\n",
        "        next_q_value = torch.mean(torch.stack(next_word_qvalues, -1), -1)  # batch\n",
        "        next_q_value = next_q_value.detach()\n",
        "\n",
        "        rewards = torch.stack(batch.reward)  # batch\n",
        "        not_done = 1.0 - np.array(batch.done, dtype='float32')  # batch\n",
        "        not_done = to_pt(not_done, self.use_cuda, type='float')\n",
        "        # NB: Should not_done be used?\n",
        "        rewards = rewards + not_done * next_q_value * (self.discount_gamma**self.nsteps)  # batch\n",
        "        #rewards = rewards + next_q_value * (self.discount_gamma**self.nsteps)  # batch\n",
        "        mask = torch.stack(batch.mask)  # batch\n",
        "        loss = F.smooth_l1_loss(q_value * mask, rewards * mask)\n",
        "        return loss\n",
        "\n",
        "    def save_agent(self):\n",
        "        model_save = os.path.join(self.model_checkpoint_dir, self.experiment_tag + \"_episode_\" + str(self.current_episode) + \".pt\")\n",
        "        memory_save = os.path.join(self.model_checkpoint_dir, self.experiment_tag + \".pickle\")\n",
        "        if not os.path.isdir(self.model_checkpoint_dir):\n",
        "            os.mkdir(self.model_checkpoint_dir)\n",
        "        torch.save(self.model.state_dict(), model_save)\n",
        "        with open(memory_save, 'wb') as f:\n",
        "            pickle.dump(self.replay_memory, f)\n",
        "        print(\"\\n========= saved checkpoint =========\")\n",
        "        \n",
        "        \n",
        "    def finish(self) -> None:\n",
        "        \"\"\"\n",
        "        All games in the batch are finished. One can choose to save checkpoints,\n",
        "        evaluate on validation set, or do parameter annealing here.\n",
        "        \"\"\"\n",
        "        # Game has finished (either win, lose, or exhausted all the given steps).\n",
        "        self.final_rewards = np.array(self.scores[-1], dtype='float32')  # batch\n",
        "        dones = []\n",
        "        for d in self.dones:\n",
        "            d = np.array([float(dd) for dd in d], dtype='float32')\n",
        "            dones.append(d)\n",
        "        dones = np.array(dones)\n",
        "        step_used = 1.0 - dones\n",
        "        self.step_used_before_done = np.sum(step_used, 0)  # batch\n",
        "        \n",
        "        self.history_avg_scores.push(np.mean(self.final_rewards))\n",
        "            \n",
        "        # save checkpoint\n",
        "        if self.mode == \"train\" and self.current_episode % self.save_frequency == 0:\n",
        "            avg_score = self.history_avg_scores.get_avg()\n",
        "            if avg_score > self.best_avg_score_so_far:\n",
        "                self.best_avg_score_so_far = avg_score\n",
        "                self.save_agent()\n",
        "\n",
        "\n",
        "        self.current_episode += 1\n",
        "        # annealing\n",
        "        if self.current_episode < self.epsilon_anneal_episodes:\n",
        "            self.epsilon -= (self.epsilon_anneal_from - self.epsilon_anneal_to) / float(self.epsilon_anneal_episodes)\n",
        "            \n",
        "    def get_mean_loss(self):\n",
        "        mean_loss = 0.\n",
        "        if len(self.loss) != 0:   \n",
        "            mean_loss = sum(self.loss) / len(self.loss)\n",
        "        self.loss = []\n",
        "        return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHuITVpD0nAN",
        "colab_type": "text"
      },
      "source": [
        "## Configs and environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9jFzEnr1zt9",
        "colab_type": "text"
      },
      "source": [
        "### Vocab\n",
        "Upload vocab.txt file`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfc78DzULeIf",
        "colab_type": "code",
        "outputId": "a1154d48-b67a-4e1e-ab4d-81fd25ed2541",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "from google.colab import files\n",
        "                                                                                                                                                                                                                                                                                                                            \n",
        "if not os.path.isfile('./vocab.txt'):\n",
        "    uploaded = files.upload()\n",
        "    # Upload vocab.txt\n",
        "    for fn in uploaded.keys():\n",
        "        print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "else:\n",
        "    print(\"Vocab already uploaded!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e8717c65-571d-438c-bf1c-46d8e2403a59\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e8717c65-571d-438c-bf1c-46d8e2403a59\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving vocab_new.txt to vocab_new.txt\n",
            "User uploaded file \"vocab_new.txt\" with length 158830 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhn6sd_gLvC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv vocab_new.txt vocab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWAhh8mL2WCf",
        "colab_type": "text"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufN2JE2f1eTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./config.yaml', 'w') as config:\n",
        "    config.write(\"\"\"\n",
        "general:\n",
        "  discount_gamma: 0.7\n",
        "  random_seed: 42\n",
        "  use_cuda: True  # disable this when running on machine without cuda\n",
        "\n",
        "  # replay memory\n",
        "  replay_memory_capacity: 10000  # adjust this depending on your RAM size\n",
        "  replay_memory_priority_fraction: 0.5\n",
        "  update_per_k_game_steps: 5\n",
        "  replay_batch_size: 32\n",
        "  nsteps: 3\n",
        "\n",
        "  # epsilon greedy\n",
        "  epsilon_anneal_episodes: 200  # -1 if not annealing\n",
        "  epsilon_anneal_from: 1\n",
        "  epsilon_anneal_to: 0.1\n",
        "\n",
        "checkpoint:\n",
        "  experiment_tag: 'lstm-ddqn-noisy'\n",
        "  model_checkpoint_dir: '/gdrive/My Drive/saved_models'\n",
        "  load_pretrained: True  # during test, enable this so that the agent load your pretrained model\n",
        "      #pretrained_experiment_dir: 'starting-kit'\n",
        "  save_frequency: 50\n",
        "\n",
        "training:\n",
        "  batch_size: 10   # Parallel games played at once\n",
        "  nb_epochs: 100\n",
        "  max_nb_steps_per_episode: 100  # after this many steps, a game is terminated\n",
        "  target_model_update_frequency: 8 # update target model after that number of backprops\n",
        "  qval_noise_std: 0.1\n",
        "  optimizer:\n",
        "    step_rule: 'adam'  # adam\n",
        "    learning_rate: 0.001\n",
        "    clip_grad_norm: 5\n",
        "\n",
        "model:\n",
        "  embedding_path: ./glove.6B.50d.txt\n",
        "  embedding_size: 50\n",
        "  noisy_std: 0.3\n",
        "  freeze_embedding: False\n",
        "  encoder_rnn_hidden_size: [128]\n",
        "  bidirectional_lstm: False\n",
        "  action_scorer_hidden_dim: 64\n",
        "  dropout_between_rnn_layers: 0.\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je184JvpCfos",
        "colab_type": "text"
      },
      "source": [
        "### Mount drive to load games\n",
        "\n",
        "Notebook takes sample games from google drive(requires authentication).\n",
        "\n",
        "To train the agent with games, upload archive with them in google drive and fix the path to the archive inside drive below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XugC3fAz9hpB",
        "colab_type": "code",
        "outputId": "4de4beab-19c3-4f83-ded5-619c4db57290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcksPU-AqFwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf starting_kit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOV193DaBuEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put your path of the games archive.\n",
        "!ls -1 | grep -q '^.*\\.ulx$' || tar -xzvf '/gdrive/My Drive/starting_kit_games.tgz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgB5jNFdoYdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hLhnwCACncb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_sample_games = './train'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924KmPsyvmwF",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2HzAOxlCPDNI",
        "colab": {}
      },
      "source": [
        "# List of additional information available during evaluation.\n",
        "AVAILABLE_INFORMATION = EnvInfos(\n",
        "    description=True, inventory=True,\n",
        "    max_score=True, objective=True, entities=True, verbs=True,\n",
        "    command_templates=True, admissible_commands=True,\n",
        "    has_won=True, has_lost=True,\n",
        "    extras=[\"recipe\"]\n",
        ")\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "def _validate_requested_infos(infos: EnvInfos):\n",
        "    msg = \"The following information cannot be requested: {}\"\n",
        "    for key in infos.basics:\n",
        "        if not getattr(AVAILABLE_INFORMATION, key):\n",
        "            raise ValueError(msg.format(key))\n",
        "\n",
        "    for key in infos.extras:\n",
        "        if key not in AVAILABLE_INFORMATION.extras:\n",
        "            raise ValueError(msg.format(key))\n",
        "            \n",
        "def get_index(game_no, stats):\n",
        "    return \"{}_{}\".format(game_no, stats)\n",
        "            \n",
        "def print_epoch_stats(epoch_no, stats):\n",
        "    print(\"\\n\\nEpoch: {:3d}\".format(epoch_no))\n",
        "    steps, scores, loss = stats[\"steps\"], stats[\"scores\"], stats[\"loss\"],\n",
        "    max_scores, outcomes = stats[\"max_score\"], stats[\"outcomes\"]\n",
        "    games_cnt, parallel_cnt = len(steps), len(steps[0])\n",
        "    columns = [ get_index(col, st) for col in range(games_cnt) for st in ['st', 'sc']]\n",
        "    stats_df = pd.DataFrame(index=list(range(parallel_cnt)) + [\"avr\", \"loss\"], columns=columns)\n",
        "        \n",
        "    for col in range(games_cnt):\n",
        "        for row in range(parallel_cnt):\n",
        "            outcome = outcomes[col][row]\n",
        "            outcome = outcome > 0 and \"W\" or outcome < 0 and \"L\" or \"\"\n",
        "            stats_df[get_index(col, 'st')][row] = steps[col][row]\n",
        "            stats_df[get_index(col, 'sc')][row] = outcome + \" \" + str(scores[col][row])\n",
        "        stats_df[get_index(col, 'sc')]['avr'] = \"{}/{}\".format(np.mean(scores[col]), max_scores[col])\n",
        "        stats_df[get_index(col, 'st')]['avr'] = stats_df[get_index(col, 'st')].mean()\n",
        "        stats_df[get_index(col, 'st')]['loss'] =  stats[\"eps\"][col]\n",
        "        stats_df[get_index(col, 'sc')]['loss'] = \"{:.5f}\".format(loss[col])\n",
        "    print(stats_df)\n",
        "    \n",
        "def get_game_id(game_info):\n",
        "    return hash((tuple(game_info['entities'][0]), game_info['extra.recipe'][0]))\n",
        "    #return hash((game_info['entities'], game_info['extra.recipe']))\n",
        "\n",
        "def make_stats(count_games):\n",
        "    stats_cols = [ \"scores\", \"steps\", \"loss\", \"max_score\", \"outcomes\", \"eps\" ]\n",
        "    stats = {}\n",
        "    for col in stats_cols:\n",
        "        stats[col] = [0] * count_games\n",
        "    return stats\n",
        "    \n",
        "def train(game_files):\n",
        "    print(\"Agent starting...\")\n",
        "    agent = CustomAgent()\n",
        "    print(\"Agent started\")\n",
        "    requested_infos = agent.select_additional_infos()\n",
        "    _validate_requested_infos(requested_infos)\n",
        "\n",
        "    env_id = textworld.gym.register_games(game_files, requested_infos,\n",
        "                                          max_episode_steps=agent.max_nb_steps_per_episode,\n",
        "                                          name=\"training\")\n",
        "    env_id = textworld.gym.make_batch(env_id, batch_size=agent.batch_size, parallel=True)\n",
        "    print(\"Making {} parallel environments to train on them\\n\".format(agent.batch_size))\n",
        "    env = gym.make(env_id)\n",
        "    count_games = len(game_files)\n",
        "    games_ids = {}\n",
        "    for epoch_no in range(1, agent.nb_epochs + 1):\n",
        "        stats = make_stats(count_games)\n",
        "        idx = 0\n",
        "        for game_no in tqdm(range(count_games)):\n",
        "            obs, infos = env.reset()\n",
        "            game_id = get_game_id(infos)\n",
        "            if epoch_no == 1:\n",
        "                games_ids[game_id] = idx\n",
        "                idx += 1\n",
        "            real_id = games_ids[game_id]\n",
        "            stats[\"max_score\"][real_id] = infos['max_score'][0]\n",
        "            \n",
        "            agent.train()\n",
        "\n",
        "            scores = [0] * len(obs) \n",
        "            dones = [False] * len(obs)\n",
        "            steps = [0] * len(obs)\n",
        "            while not all(dones):\n",
        "                # Increase step counts.\n",
        "                steps = [step + int(not done) for step, done in zip(steps, dones)]\n",
        "                commands = agent.act(obs, scores, dones, infos)\n",
        "                obs, scores, dones, infos = env.step(commands)\n",
        "\n",
        "            # Let the agent knows the game is done.\n",
        "            agent.act(obs, scores, dones, infos)\n",
        "\n",
        "            stats[\"scores\"][real_id] = scores\n",
        "            stats[\"steps\"][real_id] = steps\n",
        "            stats[\"eps\"][real_id] = agent.epsilon\n",
        "            stats[\"loss\"][real_id] = agent.get_mean_loss()\n",
        "            stats[\"outcomes\"][real_id] = [ w-l for w, l in zip(infos['has_won'], infos['has_lost'])]\n",
        "        \n",
        "        print_epoch_stats(epoch_no, stats)\n",
        "        \n",
        "    #torch.save(agent.model, './agent_model.pt')\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCw4zpIiqyGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "\n",
        "# game_dir = path_to_sample_games\n",
        "# games = []\n",
        "# if os.path.isdir(game_dir):\n",
        "#     games += glob.glob(os.path.join(game_dir, \"*.ulx\"))\n",
        "# print(\"{} games found for training.\".format(len(games)))\n",
        "\n",
        "# if len(games) != 0:\n",
        "#     train(games)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RzpgVydmrbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_sample_games = 'train'\n",
        "def take_games(game_dir, n=10):\n",
        "  games = glob.glob(os.path.join(game_dir, \"*.ulx\"))\n",
        "  games.sort(key=lambda x: len(x))\n",
        "  return games[:n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpXpH3h_mL24",
        "colab_type": "code",
        "outputId": "29c42a40-f9d9-471d-e933-242affb7e32e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# COMMENT TRAIN\n",
        "# FIX LOADING MODEL PATH\n",
        "# FIX BATCH_SIZE TO BE 10\n",
        "\n",
        "\n",
        "def eval_games(game_files):\n",
        "  agent = CustomAgent()\n",
        "  requested_infos = agent.select_additional_infos()\n",
        "  \n",
        "  env_id = textworld.gym.register_games(game_files, requested_infos,\n",
        "                                        max_episode_steps=agent.max_nb_steps_per_episode,\n",
        "                                        name=\"eval\")\n",
        "\n",
        "  env_id = textworld.gym.make_batch(env_id, batch_size=10, parallel=True)\n",
        "  print(\"ENVID: {}\".format(env_id))\n",
        "\n",
        "  print(\"Making {} parallel environments to eval on them\\n\".format(agent.batch_size))\n",
        "  env = gym.make(env_id)\n",
        "  count_games = len(game_files)\n",
        "  games_ids = {}\n",
        "\n",
        "  stats = make_stats(count_games)\n",
        "  score_sum = 0\n",
        "  steps_sum = 0\n",
        "  steps_length = count_games*10\n",
        "  for game_no in tqdm(range(count_games)):\n",
        "      obs, infos = env.reset()\n",
        "\n",
        "      agent.eval()\n",
        "\n",
        "      scores = [0] * len(obs) \n",
        "      dones = [False] * len(obs)\n",
        "      steps = [0] * len(obs)\n",
        "      while not all(dones):\n",
        "          # Increase step counts.\n",
        "          steps = [step + int(not done) for step, done in zip(steps, dones)]\n",
        "          commands = agent.act(obs, scores, dones, infos)\n",
        "          obs, scores, dones, infos = env.step(commands)\n",
        "\n",
        "      # Let the agent knows the game is done.\n",
        "      agent.act(obs, scores, dones, infos)\n",
        "      score_sum += sum(scores)\n",
        "      steps_sum += sum(steps)\n",
        "      \n",
        "  print('Max score: ', score_sum)\n",
        "  print('Mean steps: ', steps_sum / steps_length)\n",
        "\n",
        "game_dir = path_to_sample_games\n",
        "games = []\n",
        "if os.path.isdir(game_dir):\n",
        "    games += take_games(game_dir)\n",
        "    print(games)\n",
        "    \n",
        "print(\"{} games found for training.\".format(len(games)))\n",
        "\n",
        "if len(games) != 0:\n",
        "  eval_games(games)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['train/tw-cooking-recipe2-BB6F8a7uv1cq69.ulx', 'train/tw-cooking-recipe1-x5RZIbKs202tkPK.ulx', 'train/tw-cooking-recipe2-0251hW0Lu3Q3tKP.ulx', 'train/tw-cooking-recipe2-WBlZid3TEB3Cebp.ulx', 'train/tw-cooking-recipe3-dnX0IGYHJ0yCdjl.ulx', 'train/tw-cooking-recipe3-25mrSE2hL9WikrR.ulx', 'train/tw-cooking-recipe2-qx0Mfy3ue3xCxld.ulx', 'train/tw-cooking-recipe3-WDRfeP7sLn6uNYd.ulx', 'train/tw-cooking-recipe3-630RT7LOTZeC07V.ulx', 'train/tw-cooking-recipe2-ab2pHlDgC1okuEMb.ulx']\n",
            "10 games found for training.\n",
            "Creating Q-Network\n",
            "Embedding imported!\n",
            "Creating Target Network\n",
            "Embedding imported!\n",
            "LSTM_DQN(\n",
            "  (word_embedding): Embedding(\n",
            "    (embedding_layer): Embedding(20208, 50, padding_idx=0)\n",
            "  )\n",
            "  (encoder): FastUniLSTM(\n",
            "    (rnns): ModuleList(\n",
            "      (0): LSTM(50, 128)\n",
            "    )\n",
            "  )\n",
            "  (action_scorer_shared): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (action_scorers): ModuleList(\n",
            "    (0): NoisyLinear()\n",
            "    (1): NoisyLinear()\n",
            "    (2): NoisyLinear()\n",
            "    (3): NoisyLinear()\n",
            "    (4): NoisyLinear()\n",
            "  )\n",
            ")\n",
            "Total number of parameters: 14246016\n",
            "Number of trainable parameters: 14246016\n",
            "LOADING FROM: /gdrive/My Drive/saved_models/lstm-ddqn-noisy_episode_350.pt\n",
            "Failed to load model checkpoint...\n",
            "No replay memory saves to load from: /gdrive/My Drive/saved_models\n",
            "ENVID: batch10-tw-eval-v12\n",
            "Making 10 parallel environments to eval on them\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 1/10 [00:08<01:15,  8.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 2/10 [00:14<01:02,  7.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 3/10 [00:21<00:51,  7.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 4/10 [00:27<00:41,  6.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 5/10 [00:33<00:33,  6.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 6/10 [00:40<00:27,  6.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 7/10 [00:47<00:20,  6.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 80%|████████  | 8/10 [00:53<00:13,  6.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 90%|█████████ | 9/10 [00:59<00:06,  6.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 10/10 [01:10<00:00,  7.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Max score:  0\n",
            "Mean steps:  100.0\n",
            "tw-eval-v12 closed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-130:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 383, in _recv\n",
            "    raise EOFError\n",
            "EOFError\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tw-eval-v12 closed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-129:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 383, in _recv\n",
            "    raise EOFError\n",
            "EOFError\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tw-eval-v12 closed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-128:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 383, in _recv\n",
            "    raise EOFError\n",
            "EOFError\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tw-eval-v12 closed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-127:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 383, in _recv\n",
            "    raise EOFError\n",
            "EOFError\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tw-eval-v12 closed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-126:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 383, in _recv\n",
            "    raise EOFError\n",
            "EOFError\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tw-eval-v12 closed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-125:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 383, in _recv\n",
            "    raise EOFError\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j58jHLAjsQ1U",
        "colab_type": "code",
        "outputId": "31ffd7c9-6f68-459f-9752-2e6655032cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls '/gdrive/My Drive/saved_models/lstm-ddqn-noisy_episode_350.pt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/gdrive/My Drive/saved_models/lstm-ddqn-noisy_episode_350.pt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8hIulvHzNce",
        "colab_type": "text"
      },
      "source": [
        "## Save models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrIFg_ec1uvV",
        "colab_type": "code",
        "outputId": "03e28c63-a8f0-4a9b-efad-a4b0fc6258b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "!ls -lh '/gdrive/My Drive/saved_models'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 628M\n",
            "-rw------- 1 root root  80M Jul  2 15:19 imitation-textworld-lstm-ddqn-noisy-nets_episode_50.pt\n",
            "-rw------- 1 root root 2.0M Jul  3 14:09 lstm-ddqn-noisy-action-space_episode_0.pt\n",
            "-rw------- 1 root root 643K Jul  3 16:24 lstm-ddqn-noisy-action-space_episode_100.pt\n",
            "-rw------- 1 root root 643K Jul  3 16:37 lstm-ddqn-noisy-action-space_episode_150.pt\n",
            "-rw------- 1 root root 643K Jul  3 16:48 lstm-ddqn-noisy-action-space_episode_200.pt\n",
            "-rw------- 1 root root 643K Jul  3 16:58 lstm-ddqn-noisy-action-space_episode_250.pt\n",
            "-rw------- 1 root root 643K Jul  3 17:08 lstm-ddqn-noisy-action-space_episode_300.pt\n",
            "-rw------- 1 root root 643K Jul  3 17:16 lstm-ddqn-noisy-action-space_episode_350.pt\n",
            "-rw------- 1 root root 643K Jul  3 17:23 lstm-ddqn-noisy-action-space_episode_400.pt\n",
            "-rw------- 1 root root 643K Jul  3 17:29 lstm-ddqn-noisy-action-space_episode_450.pt\n",
            "-rw------- 1 root root 643K Jul  3 17:36 lstm-ddqn-noisy-action-space_episode_500.pt\n",
            "-rw------- 1 root root 643K Jul  3 16:10 lstm-ddqn-noisy-action-space_episode_50.pt\n",
            "-rw------- 1 root root 643K Jul  3 17:42 lstm-ddqn-noisy-action-space_episode_550.pt\n",
            "-rw------- 1 root root 643K Jul  3 17:50 lstm-ddqn-noisy-action-space_episode_600.pt\n",
            "-rw------- 1 root root 643K Jul  3 17:57 lstm-ddqn-noisy-action-space_episode_650.pt\n",
            "-rw------- 1 root root 643K Jul  3 18:04 lstm-ddqn-noisy-action-space_episode_700.pt\n",
            "-rw------- 1 root root 643K Jul  3 18:11 lstm-ddqn-noisy-action-space_episode_750.pt\n",
            "-rw------- 1 root root 643K Jul  3 18:18 lstm-ddqn-noisy-action-space_episode_800.pt\n",
            "-rw------- 1 root root 643K Jul  3 18:25 lstm-ddqn-noisy-action-space_episode_850.pt\n",
            "-rw------- 1 root root 643K Jul  3 18:32 lstm-ddqn-noisy-action-space_episode_900.pt\n",
            "-rw------- 1 root root 643K Jul  3 18:39 lstm-ddqn-noisy-action-space_episode_950.pt\n",
            "-rw------- 1 root root 154M Jul  2 17:40 lstm-ddqn-noisy_episode_0.pt\n",
            "-rw------- 1 root root 4.5M Jul  2 16:32 lstm-ddqn-noisy_episode_100.pt\n",
            "-rw------- 1 root root 4.5M Jul  2 16:41 lstm-ddqn-noisy_episode_150.pt\n",
            "-rw------- 1 root root 4.5M Jul  2 16:51 lstm-ddqn-noisy_episode_200.pt\n",
            "-rw------- 1 root root 4.5M Jul  2 17:00 lstm-ddqn-noisy_episode_250.pt\n",
            "-rw------- 1 root root 4.5M Jul  2 17:08 lstm-ddqn-noisy_episode_300.pt\n",
            "-rw------- 1 root root 4.5M Jul  2 17:19 lstm-ddqn-noisy_episode_350.pt\n",
            "-rw------- 1 root root 4.5M Jul  2 16:20 lstm-ddqn-noisy_episode_50.pt\n",
            "-rw------- 1 root root 112M Jul  2 10:26 starting-kit_episode_0.pt\n",
            "-rw------- 1 root root  80M Jul  2 13:56 starting-kit_episode_150.pt\n",
            "-rw------- 1 root root  80M Jul  2 14:23 starting-kit_episode_250.pt\n",
            "-rw------- 1 root root  80M Jul  2 13:30 starting-kit_episode_50.pt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}