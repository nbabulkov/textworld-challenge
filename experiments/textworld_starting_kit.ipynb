{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textworld-starting-kit.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaT3om3nMpml",
        "colab_type": "text"
      },
      "source": [
        "# Textworld starting kit notebook\n",
        "\n",
        "Model: *LSTM-DQN with replay memory*\n",
        "\n",
        "When running first: \n",
        " 1. Run the first 2 code cells(with pip installations)\n",
        " 2. Restart runtime\n",
        " 3. Continue with the next cells\n",
        "\n",
        "This is done, because there is a problem with dependencies of **textworld** and **colab**, requiring different versions of **prompt-toolkit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QVq2c31UByC",
        "colab_type": "text"
      },
      "source": [
        "## Implementations:\n",
        "* [x] Prioritized Replay Memory\n",
        "* [ ] N-step DQN\n",
        "* [ ] Double DQN\n",
        "* [ ] Dueling DQN\n",
        "* [ ] DRQN ?\n",
        "* [ ] [Rainbow Paper](https://arxiv.org/pdf/1710.02298.pdf) ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-XUxa37Z1S5",
        "colab_type": "code",
        "outputId": "9fd39fc4-ce39-4736-bfb8-3c557a7d3a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "!pip install textworld"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textworld in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: hashids>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.2.0)\n",
            "Requirement already satisfied: pillow>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from textworld) (6.0.0)\n",
            "Requirement already satisfied: gevent==1.3.5 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.3.5)\n",
            "Requirement already satisfied: selenium>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from textworld) (3.141.0)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from textworld)\n",
            "  Using cached https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: tatsu>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from textworld) (4.4.0)\n",
            "Requirement already satisfied: jericho>=1.1.5 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.2.3)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.6/dist-packages (from textworld) (2.3)\n",
            "Requirement already satisfied: pybars3>=0.9.3 in /usr/local/lib/python3.6/dist-packages (from textworld) (0.9.6)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.0.3)\n",
            "Requirement already satisfied: urwid>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from textworld) (2.0.1)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.12.3)\n",
            "Requirement already satisfied: pydot>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.3.0)\n",
            "Requirement already satisfied: greenlet==0.4.13 in /usr/local/lib/python3.6/dist-packages (from textworld) (0.4.13)\n",
            "Requirement already satisfied: pyyaml>=3.12 in /usr/local/lib/python3.6/dist-packages (from textworld) (3.13)\n",
            "Requirement already satisfied: tqdm>=4.17.1 in /usr/local/lib/python3.6/dist-packages (from textworld) (4.28.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from textworld) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from textworld) (1.16.4)\n",
            "Requirement already satisfied: gym==0.10.4 in /usr/local/lib/python3.6/dist-packages (from textworld) (0.10.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium>=3.12.0->textworld) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->textworld) (1.12.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->textworld) (0.1.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2->textworld) (4.4.0)\n",
            "Requirement already satisfied: PyMeta3>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from pybars3>=0.9.3->textworld) (0.5.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->textworld) (7.0)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->textworld) (0.15.4)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->textworld) (2.10.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->textworld) (1.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0.0->textworld) (2.19)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot>=1.2.4->textworld) (2.4.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.4->textworld) (1.3.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.4->textworld) (2.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->flask>=1.0.2->textworld) (1.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym==0.10.4->textworld) (0.16.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.4->textworld) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.4->textworld) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.4->textworld) (2.8)\n",
            "\u001b[31mERROR: ipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit\n",
            "  Found existing installation: prompt-toolkit 1.0.16\n",
            "    Uninstalling prompt-toolkit-1.0.16:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.16\n",
            "Successfully installed prompt-toolkit-2.0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPpv7-6bVOsP",
        "colab_type": "code",
        "outputId": "6240ae6b-de13-499d-a03e-7f5fb9afc9bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!pip install prompt-toolkit==1.0.16"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting prompt-toolkit==1.0.16\n",
            "  Using cached https://files.pythonhosted.org/packages/57/a8/a151b6c61718eabe6b4672b6aa760b734989316d62ec1ba4996765e602d4/prompt_toolkit-1.0.16-py3-none-any.whl\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit==1.0.16) (0.1.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit==1.0.16) (1.12.0)\n",
            "\u001b[31mERROR: textworld 1.1.1 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.16 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jupyter-console 6.0.0 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.16 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit\n",
            "  Found existing installation: prompt-toolkit 2.0.9\n",
            "    Uninstalling prompt-toolkit-2.0.9:\n",
            "      Successfully uninstalled prompt-toolkit-2.0.9\n",
            "Successfully installed prompt-toolkit-1.0.16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jemV0hR2SUIb",
        "colab_type": "code",
        "outputId": "bc233668-468b-4693-a457-769cd1347402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install torch==1.1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75fX90LVjfbT",
        "colab_type": "code",
        "outputId": "4b32fb07-628b-43b7-db9b-fcacf19a3dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import logging\n",
        "import yaml\n",
        "import copy\n",
        "import spacy\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any\n",
        "from collections import namedtuple\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gym\n",
        "import textworld.gym\n",
        "from textworld import EnvInfos\n",
        "\n",
        "# from torchviz import make_dot\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWGX1rdFtIgW",
        "colab_type": "text"
      },
      "source": [
        "## Generic functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zO94P3hjyH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_np(x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "        return x\n",
        "    return x.data.cpu().numpy()\n",
        "\n",
        "\n",
        "def to_pt(np_matrix, enable_cuda=False, type='long'):\n",
        "    if type == 'long':\n",
        "        if enable_cuda:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.LongTensor).cuda())\n",
        "        else:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.LongTensor))\n",
        "    elif type == 'float':\n",
        "        if enable_cuda:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.FloatTensor).cuda())\n",
        "        else:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.FloatTensor))\n",
        "\n",
        "\n",
        "def _words_to_ids(words, word2id):\n",
        "    ids = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            ids.append(word2id[word])\n",
        "        except KeyError:\n",
        "            ids.append(1)\n",
        "    return ids\n",
        "\n",
        "\n",
        "def preproc(s, str_type='None', tokenizer=None, lower_case=True):\n",
        "    if s is None:\n",
        "        return [\"nothing\"]\n",
        "    s = s.replace(\"\\n\", ' ')\n",
        "    if s.strip() == \"\":\n",
        "        return [\"nothing\"]\n",
        "    if str_type == 'feedback':\n",
        "        if \"$$$$$$$\" in s:\n",
        "            s = \"\"\n",
        "        if \"-=\" in s:\n",
        "            s = s.split(\"-=\")[0]\n",
        "    s = s.strip()\n",
        "    if len(s) == 0:\n",
        "        return [\"nothing\"]\n",
        "    tokens = [t.text for t in tokenizer(s)]\n",
        "    if lower_case:\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def max_len(list_of_list):\n",
        "    return max(map(len, list_of_list))\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, maxlen=None, dtype='int32', value=0.):\n",
        "    '''\n",
        "    Partially borrowed from Keras\n",
        "    # Arguments\n",
        "        sequences: list of lists where each element is a sequence\n",
        "        maxlen: int, maximum length\n",
        "        dtype: type to cast the resulting sequence.\n",
        "        value: float, value to pad the sequences to the desired value.\n",
        "    # Returns\n",
        "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
        "    '''\n",
        "    lengths = [len(s) for s in sequences]\n",
        "    nb_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if len(s) == 0:\n",
        "            continue  # empty list was found\n",
        "        # pre truncating\n",
        "        trunc = s[-maxlen:]\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "        # post padding\n",
        "        x[idx, :len(trunc)] = trunc\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HTSoK93tM6V",
        "colab_type": "text"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2VSAkD3s7Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def masked_mean(x, m=None, dim=-1):\n",
        "    \"\"\"\n",
        "        mean pooling when there're paddings\n",
        "        input:  tensor: batch x time x h\n",
        "                mask:   batch x time\n",
        "        output: tensor: batch x h\n",
        "    \"\"\"\n",
        "    if m is None:\n",
        "        return torch.mean(x, dim=dim)\n",
        "    mask_sum = torch.sum(m, dim=-1)  # batch\n",
        "    res = torch.sum(x, dim=1)  # batch x h\n",
        "    res = res / (mask_sum.unsqueeze(-1) + 1e-6)\n",
        "    return res\n",
        "\n",
        "\n",
        "class Embedding(torch.nn.Module):\n",
        "    '''\n",
        "    inputs: x:          batch x seq (x is post-padded by 0s)\n",
        "    outputs:embedding:  batch x seq x emb\n",
        "            mask:       batch x seq\n",
        "    '''\n",
        "\n",
        "    def __init__(self, embedding_size, vocab_size, enable_cuda=False):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.enable_cuda = enable_cuda\n",
        "        self.embedding_layer = torch.nn.Embedding(self.vocab_size, self.embedding_size, padding_idx=0)\n",
        "\n",
        "    def compute_mask(self, x):\n",
        "        mask = torch.ne(x, 0).float()\n",
        "        if self.enable_cuda:\n",
        "            mask = mask.cuda()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.embedding_layer(x)  # batch x time x emb\n",
        "        mask = self.compute_mask(x)  # batch x time\n",
        "        return embeddings, mask\n",
        "\n",
        "\n",
        "class FastUniLSTM(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Adapted from https://github.com/facebookresearch/DrQA/\n",
        "    now supports:   different rnn size for each layer\n",
        "                    all zero rows in batch (from time distributed layer, by reshaping certain dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ninp, nhids, dropout_between_rnn_layers=0.):\n",
        "        super(FastUniLSTM, self).__init__()\n",
        "        self.ninp = ninp\n",
        "        self.nhids = nhids\n",
        "        self.nlayers = len(self.nhids)\n",
        "        self.dropout_between_rnn_layers = dropout_between_rnn_layers\n",
        "        self.stack_rnns()\n",
        "\n",
        "    def stack_rnns(self):\n",
        "        rnns = [torch.nn.LSTM(self.ninp if i == 0 else self.nhids[i - 1],\n",
        "                              self.nhids[i],\n",
        "                              num_layers=1,\n",
        "                              bidirectional=False) for i in range(self.nlayers)]\n",
        "        self.rnns = torch.nn.ModuleList(rnns)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        def pad_(tensor, n):\n",
        "            if n > 0:\n",
        "                zero_pad = torch.autograd.Variable(torch.zeros((n,) + tensor.size()[1:]))\n",
        "                if x.is_cuda:\n",
        "                    zero_pad = zero_pad.cuda()\n",
        "                tensor = torch.cat([tensor, zero_pad])\n",
        "            return tensor\n",
        "\n",
        "        \"\"\"\n",
        "        inputs: x:          batch x time x inp\n",
        "                mask:       batch x time\n",
        "        output: encoding:   batch x time x hidden[-1]\n",
        "        \"\"\"\n",
        "        # Compute sorted sequence lengths\n",
        "        batch_size = x.size(0)\n",
        "        lengths = mask.data.eq(1).long().sum(1)  # .squeeze()\n",
        "        _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
        "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
        "\n",
        "        lengths = list(lengths[idx_sort])\n",
        "        idx_sort = torch.autograd.Variable(idx_sort)\n",
        "        idx_unsort = torch.autograd.Variable(idx_unsort)\n",
        "\n",
        "        # Sort x\n",
        "        x = x.index_select(0, idx_sort)\n",
        "\n",
        "        # remove non-zero rows, and remember how many zeros\n",
        "        n_nonzero = np.count_nonzero(lengths)\n",
        "        n_zero = batch_size - n_nonzero\n",
        "        if n_zero != 0:\n",
        "            lengths = lengths[:n_nonzero]\n",
        "            x = x[:n_nonzero]\n",
        "\n",
        "        # Transpose batch and sequence dims\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        # Pack it up\n",
        "        rnn_input = torch.nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
        "\n",
        "        # Encode all layers\n",
        "        outputs = [rnn_input]\n",
        "        for i in range(self.nlayers):\n",
        "            rnn_input = outputs[-1]\n",
        "\n",
        "            # dropout between rnn layers\n",
        "            if self.dropout_between_rnn_layers > 0:\n",
        "                dropout_input = F.dropout(rnn_input.data,\n",
        "                                          p=self.dropout_between_rnn_layers,\n",
        "                                          training=self.training)\n",
        "                rnn_input = torch.nn.utils.rnn.PackedSequence(dropout_input,\n",
        "                                                              rnn_input.batch_sizes)\n",
        "            seq, last = self.rnns[i](rnn_input)\n",
        "            outputs.append(seq)\n",
        "            if i == self.nlayers - 1:\n",
        "                # last layer\n",
        "                last_state = last[0]  # (num_layers * num_directions, batch, hidden_size)\n",
        "                last_state = last_state[0]  # batch x hidden_size\n",
        "\n",
        "        # Unpack everything\n",
        "        for i, o in enumerate(outputs[1:], 1):\n",
        "            outputs[i] = torch.nn.utils.rnn.pad_packed_sequence(o)[0]\n",
        "        output = outputs[-1]\n",
        "\n",
        "        # Transpose and unsort\n",
        "        output = output.transpose(0, 1)  # batch x time x enc\n",
        "\n",
        "        # re-padding\n",
        "        output = pad_(output, n_zero)\n",
        "        last_state = pad_(last_state, n_zero)\n",
        "\n",
        "        output = output.index_select(0, idx_unsort)\n",
        "        last_state = last_state.index_select(0, idx_unsort)\n",
        "\n",
        "        # Pad up to original batch sequence length\n",
        "        if output.size(1) != mask.size(1):\n",
        "            padding = torch.zeros(output.size(0),\n",
        "                                  mask.size(1) - output.size(1),\n",
        "                                  output.size(2)).type(output.data.type())\n",
        "            output = torch.cat([output, torch.autograd.Variable(padding)], 1)\n",
        "\n",
        "        output = output.contiguous() * mask.unsqueeze(-1)\n",
        "        return output, last_state, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0NRD6XDtRKa",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4txzFctSFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LSTM_DQN(torch.nn.Module):\n",
        "    model_name = 'lstm_dqn'\n",
        "\n",
        "    def __init__(self, model_config, word_vocab, generate_length=5, enable_cuda=False):\n",
        "        super(LSTM_DQN, self).__init__()\n",
        "        self.model_config = model_config\n",
        "        self.enable_cuda = enable_cuda\n",
        "        self.word_vocab_size = len(word_vocab)\n",
        "        self.id2word = word_vocab\n",
        "        self.generate_length = generate_length\n",
        "        self.read_config()\n",
        "        self._def_layers()\n",
        "        self.init_weights()\n",
        "        self.print_parameters()\n",
        "\n",
        "    def print_parameters(self):\n",
        "        print(self)\n",
        "        amount = 0\n",
        "        for p in self.parameters():\n",
        "            amount += np.prod(p.size())\n",
        "        print(\"total number of parameters: %s\" % (amount))\n",
        "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
        "        amount = 0\n",
        "        for p in parameters:\n",
        "            amount += np.prod(p.size())\n",
        "        print(\"number of trainable parameters: %s\" % (amount))\n",
        "\n",
        "    def read_config(self):\n",
        "        # model config\n",
        "        self.embedding_size = self.model_config['embedding_size']\n",
        "        self.encoder_rnn_hidden_size = self.model_config['encoder_rnn_hidden_size']\n",
        "        self.action_scorer_hidden_dim = self.model_config['action_scorer_hidden_dim']\n",
        "        self.dropout_between_rnn_layers = self.model_config['dropout_between_rnn_layers']\n",
        "\n",
        "    def _def_layers(self):\n",
        "\n",
        "        # word embeddings\n",
        "        self.word_embedding = Embedding(embedding_size=self.embedding_size,\n",
        "                                        vocab_size=self.word_vocab_size,\n",
        "                                        enable_cuda=self.enable_cuda)\n",
        "\n",
        "        # lstm encoder\n",
        "        self.encoder = FastUniLSTM(ninp=self.embedding_size,\n",
        "                                   nhids=self.encoder_rnn_hidden_size,\n",
        "                                   dropout_between_rnn_layers=self.dropout_between_rnn_layers)\n",
        "\n",
        "        self.action_scorer_shared = torch.nn.Linear(self.encoder_rnn_hidden_size[-1], self.action_scorer_hidden_dim)\n",
        "        # DEBUG: self.action_scorer_shared = torch.nn.LSTM(self.encoder_rnn_hidden_size[-1], self.action_scorer_hidden_dim)\n",
        "        action_scorers = []\n",
        "        for _ in range(self.generate_length):\n",
        "            action_scorers.append(torch.nn.Linear(self.action_scorer_hidden_dim, self.word_vocab_size, bias=False))\n",
        "        self.action_scorers = torch.nn.ModuleList(action_scorers)\n",
        "        self.fake_recurrent_mask = None\n",
        "\n",
        "    def init_weights(self):\n",
        "        torch.nn.init.xavier_uniform_(self.action_scorer_shared.weight.data)\n",
        "        for i in range(len(self.action_scorers)):\n",
        "            torch.nn.init.xavier_uniform_(self.action_scorers[i].weight.data)\n",
        "        self.action_scorer_shared.bias.data.fill_(0)\n",
        "\n",
        "    # DEBUG:\n",
        "    #def init_weights(self):\n",
        "    #    for name, param in self.action_scorer_shared.named_parameters():\n",
        "    #        if 'bias' in name:\n",
        "    #            torch.nn.init.constant_(param, 0.0)\n",
        "    #        elif 'weight' in name:\n",
        "    #            torch.nn.init.xavier_normal_(param)\n",
        "    #    for i in range(len(self.action_scorers)):\n",
        "    #        torch.nn.init.xavier_uniform_(self.action_scorers[i].weight.data)\n",
        "\n",
        "    def representation_generator(self, _input_words):\n",
        "        embeddings, mask = self.word_embedding.forward(_input_words)  # batch x time x emb\n",
        "        encoding_sequence, _, _ = self.encoder.forward(embeddings, mask)  # batch x time x h\n",
        "#         print(\"SIZ: {}\".format(encoding_sequence.size()))\n",
        "        mean_encoding = masked_mean(encoding_sequence, mask)  # batch x h\n",
        "        return mean_encoding\n",
        "\n",
        "    def action_scorer(self, state_representation):\n",
        "        # DEBUG: hidden = self.action_scorer_shared.forward(state_representation)[0]  # batch x hid\n",
        "        hidden = self.action_scorer_shared.forward(state_representation)  # batch x hid\n",
        "        hidden = F.relu(hidden)  # batch x hid\n",
        "        action_ranks = []\n",
        "        for i in range(len(self.action_scorers)):\n",
        "            action_ranks.append(self.action_scorers[i].forward(hidden))  # batch x n_vocab\n",
        "        return action_ranks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcGhySILtg5T",
        "colab_type": "text"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jF57tZRBzMa8",
        "colab": {}
      },
      "source": [
        "# a snapshot of state to be stored in replay memory\n",
        "Transition = namedtuple('Transition', ('observation_id_list', 'word_indices',\n",
        "                                       'reward', 'mask', 'done',\n",
        "                                       'next_observation_id_list',\n",
        "                                       'next_word_masks'))\n",
        "\n",
        "\n",
        "class HistoryScoreCache(object):\n",
        "\n",
        "    def __init__(self, capacity=1):\n",
        "        self.capacity = capacity\n",
        "        self.reset()\n",
        "\n",
        "    def push(self, stuff):\n",
        "        \"\"\"stuff is float.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(stuff)\n",
        "        else:\n",
        "            self.memory = self.memory[1:] + [stuff]\n",
        "\n",
        "    def get_avg(self):\n",
        "        return np.mean(np.array(self.memory))\n",
        "\n",
        "    def reset(self):\n",
        "        self.memory = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class PrioritizedReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity=100000, priority_fraction=0.0):\n",
        "        # prioritized replay memory\n",
        "        self.priority_fraction = priority_fraction\n",
        "        self.alpha_capacity = int(capacity * priority_fraction)\n",
        "        self.beta_capacity = capacity - self.alpha_capacity\n",
        "        self.alpha_memory, self.beta_memory = [], []\n",
        "        self.alpha_position, self.beta_position = 0, 0\n",
        "\n",
        "    def push(self, is_prior=False, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if self.priority_fraction == 0.0:\n",
        "            is_prior = False\n",
        "        if is_prior:\n",
        "            if len(self.alpha_memory) < self.alpha_capacity:\n",
        "                self.alpha_memory.append(None)\n",
        "            self.alpha_memory[self.alpha_position] = Transition(*args)\n",
        "            self.alpha_position = (self.alpha_position + 1) % self.alpha_capacity\n",
        "        else:\n",
        "            if len(self.beta_memory) < self.beta_capacity:\n",
        "                self.beta_memory.append(None)\n",
        "            self.beta_memory[self.beta_position] = Transition(*args)\n",
        "            self.beta_position = (self.beta_position + 1) % self.beta_capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if self.priority_fraction == 0.0:\n",
        "            from_beta = min(batch_size, len(self.beta_memory))\n",
        "            res = random.sample(self.beta_memory, from_beta)\n",
        "        else:\n",
        "            from_alpha = min(int(self.priority_fraction * batch_size), len(self.alpha_memory))\n",
        "            from_beta = min(batch_size - int(self.priority_fraction * batch_size), len(self.beta_memory))\n",
        "            res = random.sample(self.alpha_memory, from_alpha) + random.sample(self.beta_memory, from_beta)\n",
        "        random.shuffle(res)\n",
        "        return res\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.alpha_memory) + len(self.beta_memory)\n",
        "\n",
        "\n",
        "class CustomAgent:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            word_vocab: List of words supported.\n",
        "        \"\"\"\n",
        "        self.mode = \"train\"\n",
        "        with open(\"./vocab.txt\") as f:\n",
        "            self.word_vocab = f.read().split(\"\\n\")\n",
        "        with open(\"config.yaml\") as reader:\n",
        "            self.config = yaml.safe_load(reader)\n",
        "        self.word2id = { val: idx for idx, val in enumerate(self.word_vocab) }\n",
        "        self.EOS_id = self.word2id[\"</S>\"]\n",
        "\n",
        "        self.batch_size = self.config['training']['batch_size']\n",
        "        self.max_nb_steps_per_episode = self.config['training']['max_nb_steps_per_episode']\n",
        "        self.nb_epochs = self.config['training']['nb_epochs']\n",
        "\n",
        "        # Set the random seed manually for reproducibility.\n",
        "        np.random.seed(self.config['general']['random_seed'])\n",
        "        torch.manual_seed(self.config['general']['random_seed'])\n",
        "        if torch.cuda.is_available():\n",
        "            if not self.config['general']['use_cuda']:\n",
        "                print(\"WARNING: CUDA device detected but 'use_cuda: false' found in config.yaml\")\n",
        "                self.use_cuda = False\n",
        "            else:\n",
        "                torch.backends.cudnn.deterministic = True\n",
        "                torch.cuda.manual_seed(self.config['general']['random_seed'])\n",
        "                self.use_cuda = True\n",
        "        else:\n",
        "            self.use_cuda = False\n",
        "\n",
        "        self.model = LSTM_DQN(model_config=self.config[\"model\"],\n",
        "                              word_vocab=self.word_vocab,\n",
        "                              enable_cuda=self.use_cuda)\n",
        "\n",
        "        self.experiment_tag = self.config['checkpoint']['experiment_tag']\n",
        "        self.model_checkpoint_path = self.config['checkpoint']['model_checkpoint_path']\n",
        "        self.save_frequency = self.config['checkpoint']['save_frequency']\n",
        "\n",
        "        if self.config['checkpoint']['load_pretrained']:\n",
        "            self.load_pretrained_model(self.model_checkpoint_path + '/' + self.config['checkpoint']['pretrained_experiment_tag'] + '.pt')\n",
        "        if self.use_cuda:\n",
        "            self.model.cuda()\n",
        "\n",
        "        self.replay_batch_size = self.config['general']['replay_batch_size']\n",
        "        self.replay_memory = PrioritizedReplayMemory(self.config['general']['replay_memory_capacity'],\n",
        "                                                     priority_fraction=self.config['general']['replay_memory_priority_fraction'])\n",
        "\n",
        "        # optimizer\n",
        "        parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "        self.optimizer = torch.optim.Adam(parameters, lr=self.config['training']['optimizer']['learning_rate'])\n",
        "\n",
        "        # epsilon greedy\n",
        "        self.epsilon_anneal_episodes = self.config['general']['epsilon_anneal_episodes']\n",
        "        self.epsilon_anneal_from = self.config['general']['epsilon_anneal_from']\n",
        "        self.epsilon_anneal_to = self.config['general']['epsilon_anneal_to']\n",
        "        self.epsilon = self.epsilon_anneal_from\n",
        "        self.update_per_k_game_steps = self.config['general']['update_per_k_game_steps']\n",
        "        self.clip_grad_norm = self.config['training']['optimizer']['clip_grad_norm']\n",
        "\n",
        "        self.nlp = spacy.load('en', disable=['ner', 'parser', 'tagger'])\n",
        "        self.preposition_map = {\"take\": \"from\",\n",
        "                                \"chop\": \"with\",\n",
        "                                \"slice\": \"with\",\n",
        "                                \"dice\": \"with\",\n",
        "                                \"cook\": \"with\",\n",
        "                                \"insert\": \"into\",\n",
        "                                \"put\": \"on\"}\n",
        "        self.single_word_verbs = set([\"inventory\", \"look\"])\n",
        "        self.discount_gamma = self.config['general']['discount_gamma']\n",
        "        self.current_episode = 0\n",
        "        self.current_step = 0\n",
        "        self._epsiode_has_started = False\n",
        "        self.history_avg_scores = HistoryScoreCache(capacity=1000)\n",
        "        self.best_avg_score_so_far = 0.0\n",
        "        self.loss = []\n",
        "        self.state = ''\n",
        "\n",
        "    def train(self, imitate = False):\n",
        "        \"\"\"\n",
        "        Tell the agent that it's training phase.\n",
        "        \"\"\"\n",
        "        self.mode = \"train\"\n",
        "        self.imitate = imitate\n",
        "        self.wt_index = 0\n",
        "#         print(self.wt_index)\n",
        "        self.model.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        Tell the agent that it's evaluation phase.\n",
        "        \"\"\"\n",
        "        self.mode = \"eval\"\n",
        "        self.model.eval()\n",
        "\n",
        "    def _start_episode(self, obs: List[str], infos: Dict[str, List[Any]]) -> None:\n",
        "        \"\"\"\n",
        "        Prepare the agent for the upcoming episode.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Initial feedback for each game.\n",
        "            infos: Additional information for each game.\n",
        "        \"\"\"\n",
        "        self.init(obs, infos)\n",
        "        self._epsiode_has_started = True\n",
        "\n",
        "    def _end_episode(self, obs: List[str], scores: List[int], infos: Dict[str, List[Any]]) -> None:\n",
        "        \"\"\"\n",
        "        Tell the agent the episode has terminated.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            score: The score obtained so far for each game.\n",
        "            infos: Additional information for each game.\n",
        "        \"\"\"\n",
        "        self.finish()\n",
        "        self._epsiode_has_started = False\n",
        "\n",
        "    def load_pretrained_model(self, load_from):\n",
        "        \"\"\"\n",
        "        Load pretrained checkpoint from file.\n",
        "\n",
        "        Arguments:\n",
        "            load_from: File name of the pretrained model checkpoint.\n",
        "        \"\"\"\n",
        "        print(\"loading model from %s\\n\" % (load_from))\n",
        "        try:\n",
        "            if self.use_cuda:\n",
        "                state_dict = torch.load(load_from)\n",
        "            else:\n",
        "                state_dict = torch.load(load_from, map_location='cpu')\n",
        "            self.model.load_state_dict(state_dict)\n",
        "        except:\n",
        "            print(\"Failed to load checkpoint...\")\n",
        "\n",
        "    def init(self, obs: List[str], infos: Dict[str, List[Any]]):\n",
        "        \"\"\"\n",
        "        Prepare the agent for the upcoming games.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            infos: Additional information for each game.\n",
        "        \"\"\"\n",
        "        # reset agent, get vocabulary masks for verbs / adjectives / nouns\n",
        "        self.scores = []\n",
        "        self.dones = []\n",
        "        self.prev_actions = [\"\" for _ in range(len(obs))]\n",
        "        # get word masks\n",
        "        batch_size = len(infos[\"verbs\"])\n",
        "        verbs_word_list = infos[\"verbs\"]\n",
        "        noun_word_list, adj_word_list = [], []\n",
        "        for entities in infos[\"entities\"]:\n",
        "            tmp_nouns, tmp_adjs = [], []\n",
        "            for name in entities:\n",
        "                split = name.split()\n",
        "                tmp_nouns.append(split[-1])\n",
        "                if len(split) > 1:\n",
        "                    tmp_adjs.append(\" \".join(split[:-1]))\n",
        "            noun_word_list.append(list(set(tmp_nouns)))\n",
        "            adj_word_list.append(list(set(tmp_adjs)))\n",
        "        \n",
        "        verb_mask = np.zeros((batch_size, len(self.word_vocab)), dtype=\"float32\")\n",
        "        noun_mask = np.zeros((batch_size, len(self.word_vocab)), dtype=\"float32\")\n",
        "        adj_mask = np.zeros((batch_size, len(self.word_vocab)), dtype=\"float32\")\n",
        "        for i in range(batch_size):\n",
        "            for w in verbs_word_list[i]:\n",
        "                if w in self.word2id:\n",
        "                    verb_mask[i][self.word2id[w]] = 1.0\n",
        "            for w in noun_word_list[i]:\n",
        "                if w in self.word2id:\n",
        "                    noun_mask[i][self.word2id[w]] = 1.0\n",
        "            for w in adj_word_list[i]:\n",
        "                if w not in self.word2id:\n",
        "                    adj_mask[i][self.word2id[w]] = 1.0\n",
        "                \n",
        "        second_noun_mask = copy.copy(noun_mask)\n",
        "        second_adj_mask = copy.copy(adj_mask)\n",
        "        second_noun_mask[:, self.EOS_id] = 1.0\n",
        "        adj_mask[:, self.EOS_id] = 1.0\n",
        "        second_adj_mask[:, self.EOS_id] = 1.0\n",
        "        self.word_masks_np = [verb_mask, adj_mask, noun_mask, second_adj_mask, second_noun_mask]\n",
        "\n",
        "        self.cache_description_id_list = None\n",
        "        self.cache_chosen_indices = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def get_game_step_info(self, obs: List[str], infos: Dict[str, List[Any]]):\n",
        "        \"\"\"\n",
        "        Get all the available information, and concat them together to be tensor for\n",
        "        a neural model. we use post padding here, all information are tokenized here.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            infos: Additional information for each game.\n",
        "        \"\"\"\n",
        "        inventory_token_list = [preproc(item, tokenizer=self.nlp) for item in infos[\"inventory\"]]\n",
        "        inventory_id_list = [_words_to_ids(tokens, self.word2id) for tokens in inventory_token_list]\n",
        "\n",
        "        feedback_token_list = [preproc(item, str_type='feedback', tokenizer=self.nlp) for item in obs]\n",
        "        feedback_id_list = [_words_to_ids(tokens, self.word2id) for tokens in feedback_token_list]\n",
        "\n",
        "        quest_token_list = [preproc(item, tokenizer=self.nlp) for item in infos[\"extra.recipe\"]]\n",
        "        quest_id_list = [_words_to_ids(tokens, self.word2id) for tokens in quest_token_list]\n",
        "\n",
        "        prev_action_token_list = [preproc(item, tokenizer=self.nlp) for item in self.prev_actions]\n",
        "        prev_action_id_list = [_words_to_ids(tokens, self.word2id) for tokens in prev_action_token_list]\n",
        "\n",
        "        description_token_list = [preproc(item, tokenizer=self.nlp) for item in infos[\"description\"]]\n",
        "        for i, d in enumerate(description_token_list):\n",
        "            if len(d) == 0:\n",
        "                description_token_list[i] = [\"end\"]  # if empty description, insert word \"end\"\n",
        "        description_id_list = [_words_to_ids(tokens, self.word2id) for tokens in description_token_list]\n",
        "        description_id_list = [_d + _i + _q + _f + _pa for (_d, _i, _q, _f, _pa) in zip(description_id_list, inventory_id_list, quest_id_list, feedback_id_list, prev_action_id_list)]\n",
        "\n",
        "        input_description = pad_sequences(description_id_list, maxlen=max_len(description_id_list)).astype('int32')\n",
        "        input_description = to_pt(input_description, self.use_cuda)\n",
        "\n",
        "        return input_description, description_id_list\n",
        "\n",
        "    def word_ids_to_commands(self, verb, adj, noun, adj_2, noun_2):\n",
        "        \"\"\"\n",
        "        Turn the 5 indices into actual command strings.\n",
        "\n",
        "        Arguments:\n",
        "            verb: Index of the guessing verb in vocabulary\n",
        "            adj: Index of the guessing adjective in vocabulary\n",
        "            noun: Index of the guessing noun in vocabulary\n",
        "            adj_2: Index of the second guessing adjective in vocabulary\n",
        "            noun_2: Index of the second guessing noun in vocabulary\n",
        "        \"\"\"\n",
        "        # turns 5 indices into actual command strings\n",
        "        if self.word_vocab[verb] in self.single_word_verbs:\n",
        "            return self.word_vocab[verb]\n",
        "        if adj == self.EOS_id:\n",
        "            res = self.word_vocab[verb] + \" \" + self.word_vocab[noun]\n",
        "        else:\n",
        "            res = self.word_vocab[verb] + \" \" + self.word_vocab[adj] + \" \" + self.word_vocab[noun]\n",
        "        if self.word_vocab[verb] not in self.preposition_map:\n",
        "            return res\n",
        "        if noun_2 == self.EOS_id:\n",
        "            return res\n",
        "        prep = self.preposition_map[self.word_vocab[verb]]\n",
        "        if adj_2 == self.EOS_id:\n",
        "            res = res + \" \" + prep + \" \" + self.word_vocab[noun_2]\n",
        "        else:\n",
        "            res =  res + \" \" + prep + \" \" + self.word_vocab[adj_2] + \" \" + self.word_vocab[noun_2]\n",
        "        return res\n",
        "\n",
        "    def get_chosen_strings(self, chosen_indices):\n",
        "        \"\"\"\n",
        "        Turns list of word indices into actual command strings.\n",
        "\n",
        "        Arguments:\n",
        "            chosen_indices: Word indices chosen by model.\n",
        "        \"\"\"\n",
        "        chosen_indices_np = [to_np(item)[:, 0] for item in chosen_indices]\n",
        "        res_str = []\n",
        "        batch_size = chosen_indices_np[0].shape[0]\n",
        "        for i in range(batch_size):\n",
        "            verb, adj, noun, adj_2, noun_2 = chosen_indices_np[0][i],\\\n",
        "                                             chosen_indices_np[1][i],\\\n",
        "                                             chosen_indices_np[2][i],\\\n",
        "                                             chosen_indices_np[3][i],\\\n",
        "                                             chosen_indices_np[4][i]\n",
        "            res_str.append(self.word_ids_to_commands(verb, adj, noun, adj_2, noun_2))\n",
        "        return res_str\n",
        "\n",
        "    def get_wordid_from_vocab(self, word):\n",
        "      if word in self.word2id.keys():\n",
        "        return self.word2id[word]\n",
        "      else:\n",
        "        return self.EOS_id\n",
        "      \n",
        "    def command_to_word_ids(self, cmd, batch_size):\n",
        "      verb_id=self.EOS_id\n",
        "      first_adj=self.EOS_id\n",
        "      first_noun=self.EOS_id\n",
        "      second_adj=self.EOS_id\n",
        "      second_noun=self.EOS_id\n",
        "      \n",
        "#       print('cmd_to_ids')\n",
        "#       print(cmd.split())\n",
        "      ids = _words_to_ids(cmd.split(), self.word2id)\n",
        "#       print(ids)\n",
        "      for ind, i in enumerate(ids):\n",
        "        if self.word_masks_np[0][0][i]==1.0:\n",
        "          verb = ind\n",
        "          verb_id = i\n",
        "      nouns=[]\n",
        "      for ind, i in enumerate(ids):\n",
        "        if self.word_masks_np[2][0][i]==1.0:\n",
        "          nouns.append((ind,i))\n",
        "      if len(nouns) > 0:\n",
        "        if nouns[0][0] != verb - 1:\n",
        "          adj_ids = ids[verb + 1: nouns[0][0]]\n",
        "          adj=''\n",
        "          adj= ' '.join([self.word_vocab[x] for x in adj_ids]) \n",
        "#           print(adj)\n",
        "          first_adj=self.get_wordid_from_vocab(adj)\n",
        "#         print(nouns)\n",
        "        first_noun=nouns[0][1]\n",
        "      \n",
        "      if len(nouns) > 1:\n",
        "        if nouns[1][0] != nouns[0][0] - 1:\n",
        "          adj_ids = ids[nouns[0][0]: nouns[1][0]]\n",
        "          adj= ' '.join([self.word_vocab[x] for x in adj_ids]) \n",
        "          second_adj=self.get_wordid_from_vocab(adj)\n",
        "        second_noun=nouns[1][1]\n",
        "        \n",
        "       \n",
        "      list_ids = [verb_id, first_adj, first_noun, second_adj, second_noun]\n",
        "      return [to_pt(np.array([[x]]*batch_size), self.use_cuda) for x in list_ids]  \n",
        "      \n",
        "    def get_chosen_strings(self, chosen_indices):\n",
        "        \"\"\"\n",
        "        Turns list of word indices into actual command strings.\n",
        "\n",
        "        Arguments:\n",
        "            chosen_indices: Word indices chosen by model.\n",
        "        \"\"\"\n",
        "        chosen_indices_np = [to_np(item)[:, 0] for item in chosen_indices]\n",
        "        res_str = []\n",
        "        batch_size = chosen_indices_np[0].shape[0]\n",
        "        for i in range(batch_size):\n",
        "            verb, adj, noun, adj_2, noun_2 = chosen_indices_np[0][i],\\\n",
        "                                             chosen_indices_np[1][i],\\\n",
        "                                             chosen_indices_np[2][i],\\\n",
        "                                             chosen_indices_np[3][i],\\\n",
        "                                             chosen_indices_np[4][i]\n",
        "            res_str.append(self.word_ids_to_commands(verb, adj, noun, adj_2, noun_2))\n",
        "        return res_str\n",
        "      \n",
        "    def choose_random_command(self, word_ranks, word_masks_np):\n",
        "        \"\"\"\n",
        "        Generate a command randomly, for epsilon greedy.\n",
        "\n",
        "        Arguments:\n",
        "            word_ranks: Q values for each word by model.action_scorer.\n",
        "            word_masks_np: Vocabulary masks for words depending on their type (verb, adj, noun).\n",
        "        \"\"\"\n",
        "        batch_size = word_ranks[0].size(0)\n",
        "        word_ranks_np = [to_np(item) for item in word_ranks]  # list of batch x n_vocab\n",
        "        word_ranks_np = [r * m for r, m in zip(word_ranks_np, word_masks_np)]  # list of batch x n_vocab\n",
        "        word_indices = []\n",
        "        for i in range(len(word_ranks_np)):\n",
        "            indices = []\n",
        "            for j in range(batch_size):\n",
        "                msk = word_masks_np[i][j]  # vocab\n",
        "                indices.append(np.random.choice(len(msk), p=msk / np.sum(msk, -1)))\n",
        "            word_indices.append(np.array(indices))\n",
        "        # word_indices: list of batch\n",
        "        word_qvalues = [[] for _ in word_masks_np]\n",
        "        for i in range(batch_size):\n",
        "            for j in range(len(word_qvalues)):\n",
        "                word_qvalues[j].append(word_ranks[j][i][word_indices[j][i]])\n",
        "        word_qvalues = [torch.stack(item) for item in word_qvalues]\n",
        "        word_indices = [to_pt(item, self.use_cuda) for item in word_indices]\n",
        "        word_indices = [item.unsqueeze(-1) for item in word_indices]  # list of batch x 1\n",
        "        return word_qvalues, word_indices\n",
        "\n",
        "    def choose_maxQ_command(self, word_ranks, word_masks_np):\n",
        "        \"\"\"\n",
        "        Generate a command by maximum q values, for epsilon greedy.\n",
        "\n",
        "        Arguments:\n",
        "            word_ranks: Q values for each word by model.action_scorer.\n",
        "            word_masks_np: Vocabulary masks for words depending on their type (verb, adj, noun).\n",
        "        \"\"\"\n",
        "        batch_size = word_ranks[0].size(0)\n",
        "        word_ranks_np = [to_np(item) for item in word_ranks]  # list of batch x n_vocab\n",
        "        word_ranks_np = [r - np.min(r) for r in word_ranks_np] # minus the min value, so that all values are non-negative\n",
        "        word_ranks_np = [r * m for r, m in zip(word_ranks_np, word_masks_np)]  # list of batch x n_vocab\n",
        "        word_indices = [np.argmax(item, -1) for item in word_ranks_np]  # list of batch\n",
        "        word_qvalues = [[] for _ in word_masks_np]\n",
        "        #print(\"\\n\" + str(word_indices))\n",
        "        for i in range(batch_size):\n",
        "            for j in range(len(word_qvalues)):\n",
        "                word_qvalues[j].append(word_ranks[j][i][word_indices[j][i]])\n",
        "                #DEBUG: \n",
        "                #idx = word_indices[j][i]\n",
        "                #rank = word_ranks_np[j][i][idx]\n",
        "                #word_qvalues[j].append(rank)\n",
        "        word_qvalues = [torch.stack(item) for item in word_qvalues]\n",
        "        word_indices = [to_pt(item, self.use_cuda) for item in word_indices]\n",
        "        word_indices = [item.unsqueeze(-1) for item in word_indices]  # list of batch x 1\n",
        "        return word_qvalues, word_indices\n",
        "\n",
        "    def get_ranks(self, input_description):\n",
        "        \"\"\"\n",
        "        Given input description tensor, call model forward, to get Q values of words.\n",
        "\n",
        "        Arguments:\n",
        "            input_description: Input tensors, which include all the information chosen in\n",
        "            select_additional_infos() concatenated together.\n",
        "        \"\"\"\n",
        "        state_representation = self.model.representation_generator(input_description)\n",
        "        ## DEBUG: added\n",
        "        #dims = state_representation.size()\n",
        "        #Lstate_representation = state_representation.view(dims[0], 1, dims[-1])\n",
        "        ##\n",
        "        word_ranks = self.model.action_scorer(state_representation)  # each element in list has batch x n_vocab size\n",
        "        return word_ranks\n",
        "    \n",
        "    \n",
        "\n",
        "    def act_eval(self, obs: List[str], scores: List[int], dones: List[bool], infos: Dict[str, List[Any]]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Acts upon the current list of observations, during evaluation.\n",
        "\n",
        "        One text command must be returned for each observation.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            score: The score obtained so far for each game (at previous step).\n",
        "            done: Whether a game is finished (at previous step).\n",
        "            infos: Additional information for each game.\n",
        "\n",
        "        Returns:\n",
        "            Text commands to be performed (one per observation).\n",
        "\n",
        "        Notes:\n",
        "            Commands returned for games marked as `done` have no effect.\n",
        "            The states for finished games are simply copy over until all\n",
        "            games are done, in which case `CustomAgent.finish()` is called\n",
        "            instead.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.current_step > 0:\n",
        "            # append scores / dones from previous step into memory\n",
        "            self.scores.append(scores)\n",
        "            self.dones.append(dones)\n",
        "\n",
        "        if all(dones):\n",
        "            self._end_episode(obs, scores, infos)\n",
        "            return  # Nothing to return.\n",
        "\n",
        "        input_description, _ = self.get_game_step_info(obs, infos)\n",
        "        word_ranks = self.get_ranks(input_description)  # list of batch x vocab\n",
        "        _, word_indices_maxq = self.choose_maxQ_command(word_ranks, self.word_masks_np)\n",
        "\n",
        "        chosen_indices = word_indices_maxq\n",
        "        chosen_indices = [item.detach() for item in chosen_indices]\n",
        "        chosen_strings = self.get_chosen_strings(chosen_indices)\n",
        "        self.prev_actions = chosen_strings\n",
        "        self.current_step += 1\n",
        "\n",
        "        return chosen_strings\n",
        "\n",
        "    def act(self, obs: List[str], scores: List[int], dones: List[bool], infos: Dict[str, List[Any]]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Acts upon the current list of observations.\n",
        "\n",
        "        One text command must be returned for each observation.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            score: The score obtained so far for each game (at previous step).\n",
        "            done: Whether a game is finished (at previous step).\n",
        "            infos: Additional information for each game.\n",
        "\n",
        "        Returns:\n",
        "            Text commands to be performed (one per observation).\n",
        "\n",
        "        Notes:\n",
        "            Commands returned for games marked as `done` have no effect.\n",
        "            The states for finished games are simply copy over until all\n",
        "            games are done, in which case `CustomAgent.finish()` is called\n",
        "            instead.\n",
        "        \"\"\"\n",
        "        if not self._epsiode_has_started:\n",
        "            self._start_episode(obs, infos)\n",
        "\n",
        "        if self.mode == \"eval\":\n",
        "            return self.act_eval(obs, scores, dones, infos)\n",
        "\n",
        "        if self.current_step > 0:\n",
        "            # append scores / dones from previous step into memory\n",
        "            self.scores.append(scores)\n",
        "            self.dones.append(dones)\n",
        "            # compute previous step's rewards and masks\n",
        "            rewards_np, rewards, mask_np, mask = self.compute_reward()\n",
        "            # PRIOR\n",
        "            #scores_np = np.array(scores)\n",
        "            #if np.any(scores_np - rewards_np):\n",
        "            #    print(\"\\n Rewards: {}\".format(rewards_np))\n",
        "            #    print(\"Scores:{}\".format(scores))\n",
        "\n",
        "        input_description, description_id_list = self.get_game_step_info(obs, infos)\n",
        "        # generate commands for one game step, epsilon greedy is applied, i.e.,\n",
        "        # there is epsilon of chance to generate random commands\n",
        "        if self.imitate:\n",
        "          print('imitate')\n",
        "          correct_cmd=infos['extra.walkthrough'][0][self.wt_index]\n",
        "          print(correct_cmd)\n",
        "          if self.wt_index != len(infos['extra.walkthrough'][0]) - 1:\n",
        "            self.wt_index+=1\n",
        "          chosen_indices = self.command_to_word_ids(correct_cmd, len(input_description))\n",
        "        else:\n",
        "          word_ranks = self.get_ranks(input_description) # list of batch x vocab\n",
        "          _, word_indices_maxq = self.choose_maxQ_command(word_ranks, self.word_masks_np)\n",
        "          _, word_indices_random = self.choose_random_command(word_ranks, self.word_masks_np)\n",
        "          # random number for epsilon greedy\n",
        "          rand_num = np.random.uniform(low=0.0, high=1.0, size=(input_description.size(0), 1))\n",
        "          less_than_epsilon = (rand_num < self.epsilon).astype(\"float32\")  # batch\n",
        "          greater_than_epsilon = 1.0 - less_than_epsilon\n",
        "          less_than_epsilon = to_pt(less_than_epsilon, self.use_cuda, type='float')\n",
        "          greater_than_epsilon = to_pt(greater_than_epsilon, self.use_cuda, type='float')\n",
        "          less_than_epsilon, greater_than_epsilon = less_than_epsilon.long(), greater_than_epsilon.long()\n",
        "\n",
        "          chosen_indices = [less_than_epsilon * idx_random + greater_than_epsilon * idx_maxq for idx_random, idx_maxq in zip(word_indices_random, word_indices_maxq)]\n",
        "          chosen_indices = [item.detach() for item in chosen_indices]\n",
        "          \n",
        "        chosen_strings = self.get_chosen_strings(chosen_indices)\n",
        "        self.prev_actions = chosen_strings\n",
        "\n",
        "        # push info from previous game step into replay memory\n",
        "        if self.current_step > 0:\n",
        "            for b in range(len(obs)):\n",
        "                if mask_np[b] == 0:\n",
        "                    continue\n",
        "                is_prior = rewards_np[b] > 0.0\n",
        "                self.replay_memory.push(is_prior, self.cache_description_id_list[b], [item[b] for item in self.cache_chosen_indices], rewards[b], mask[b], dones[b], description_id_list[b], [item[b] for item in self.word_masks_np])\n",
        "\n",
        "        # cache new info in current game step into caches\n",
        "        self.cache_description_id_list = description_id_list\n",
        "        self.cache_chosen_indices = chosen_indices\n",
        "\n",
        "        # update neural model by replaying snapshots in replay memory\n",
        "        if self.current_step > 0 and self.current_step % self.update_per_k_game_steps == 0:\n",
        "            loss = self.update()\n",
        "            if loss is not None:\n",
        "                # Backpropagate\n",
        "                self.loss.append(to_np(loss).mean())\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward(retain_graph=True)\n",
        "                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad_norm)\n",
        "                self.optimizer.step()  # apply gradients\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        if all(dones):\n",
        "            self._end_episode(obs, scores, infos)\n",
        "            return  # Nothing to return.\n",
        "        return chosen_strings\n",
        "\n",
        "    def compute_reward(self):\n",
        "        \"\"\"\n",
        "        Compute rewards by agent. Note this is different from what the training/evaluation\n",
        "        scripts do. Agent keeps track of scores and other game information for training purpose.\n",
        "\n",
        "        \"\"\"\n",
        "        # mask = 1 if game is not finished or just finished at current step\n",
        "        if len(self.dones) == 1:\n",
        "            # it's not possible to finish a game at 0th step\n",
        "            mask = [1.0 for _ in self.dones[-1]]\n",
        "        else:\n",
        "            assert len(self.dones) > 1\n",
        "            mask = [1.0 if not self.dones[-2][i] else 0.0 for i in range(len(self.dones[-1]))]\n",
        "        mask = np.array(mask, dtype='float32')\n",
        "        mask_pt = to_pt(mask, self.use_cuda, type='float')\n",
        "        # rewards returned by game engine are always accumulated value the\n",
        "        # agent have recieved. so the reward it gets in the current game step\n",
        "        # is the new value minus values at previous step.\n",
        "        rewards = np.array(self.scores[-1], dtype='float32')  # batch\n",
        "        if len(self.scores) > 1:\n",
        "            prev_rewards = np.array(self.scores[-2], dtype='float32')\n",
        "            rewards = rewards - prev_rewards\n",
        "        rewards_pt = to_pt(rewards, self.use_cuda, type='float')\n",
        "\n",
        "        return rewards, rewards_pt, mask, mask_pt\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        Update neural model in agent. In this example we follow algorithm\n",
        "        of updating model in dqn with replay memory.\n",
        "\n",
        "        \"\"\"\n",
        "        if len(self.replay_memory) < self.replay_batch_size:\n",
        "            return None\n",
        "        transitions = self.replay_memory.sample(self.replay_batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        observation_id_list = pad_sequences(batch.observation_id_list, maxlen=max_len(batch.observation_id_list)).astype('int32')\n",
        "        input_observation = to_pt(observation_id_list, self.use_cuda)\n",
        "        next_observation_id_list = pad_sequences(batch.next_observation_id_list, maxlen=max_len(batch.next_observation_id_list)).astype('int32')\n",
        "        next_input_observation = to_pt(next_observation_id_list, self.use_cuda)\n",
        "        chosen_indices = list(list(zip(*batch.word_indices)))\n",
        "        chosen_indices = [torch.stack(item, 0) for item in chosen_indices]  # list of batch x 1\n",
        "\n",
        "        word_ranks = self.get_ranks(input_observation)  # list of batch x vocab\n",
        "#         print(\"WORD RANKS: {}\\n\".format([ i.size() for i in word_ranks]))\n",
        "#         print(\"IDXES: {}\\n\".format([ i.size() for i in chosen_indices]))\n",
        "        word_qvalues = [w_rank.gather(1, idx).squeeze(-1) for w_rank, idx in zip(word_ranks, chosen_indices)]  # list of batch\n",
        "        q_value = torch.mean(torch.stack(word_qvalues, -1), -1)  # batch\n",
        "\n",
        "        next_word_ranks = self.get_ranks(next_input_observation)  # batch x n_verb, batch x n_noun, batchx n_second_noun\n",
        "        next_word_masks = list(list(zip(*batch.next_word_masks)))\n",
        "        next_word_masks = [np.stack(item, 0) for item in next_word_masks]\n",
        "        next_word_qvalues, _ = self.choose_maxQ_command(next_word_ranks, next_word_masks)\n",
        "        next_q_value = torch.mean(torch.stack(next_word_qvalues, -1), -1)  # batch\n",
        "        next_q_value = next_q_value.detach()\n",
        "\n",
        "        rewards = torch.stack(batch.reward)  # batch\n",
        "        not_done = 1.0 - np.array(batch.done, dtype='float32')  # batch\n",
        "        not_done = to_pt(not_done, self.use_cuda, type='float')\n",
        "        # NOTE: shoul not_don be used?\n",
        "        rewards = rewards + not_done * next_q_value * self.discount_gamma  # batch\n",
        "        mask = torch.stack(batch.mask)  # batch\n",
        "        loss = F.smooth_l1_loss(q_value * mask, rewards * mask)\n",
        "        return loss\n",
        "\n",
        "    def finish(self) -> None:\n",
        "        \"\"\"\n",
        "        All games in the batch are finished. One can choose to save checkpoints,\n",
        "        evaluate on validation set, or do parameter annealing here.\n",
        "\n",
        "        \"\"\"\n",
        "        # Game has finished (either win, lose, or exhausted all the given steps).\n",
        "        self.final_rewards = np.array(self.scores[-1], dtype='float32')  # batch\n",
        "        dones = []\n",
        "        for d in self.dones:\n",
        "            d = np.array([float(dd) for dd in d], dtype='float32')\n",
        "            dones.append(d)\n",
        "        dones = np.array(dones)\n",
        "        step_used = 1.0 - dones\n",
        "        self.step_used_before_done = np.sum(step_used, 0)  # batch\n",
        "\n",
        "        self.history_avg_scores.push(np.mean(self.final_rewards))\n",
        "        # save checkpoint\n",
        "        if self.mode == \"train\" and self.current_episode % self.save_frequency == 0:\n",
        "            avg_score = self.history_avg_scores.get_avg()\n",
        "            if avg_score > self.best_avg_score_so_far:\n",
        "                self.best_avg_score_so_far = avg_score\n",
        "\n",
        "                save_to = self.model_checkpoint_path + '/' + self.experiment_tag + \"_episode_\" + str(self.current_episode) + \".pt\"\n",
        "                if not os.path.isdir(self.model_checkpoint_path):\n",
        "                    os.mkdir(self.model_checkpoint_path)\n",
        "                torch.save(self.model.state_dict(), save_to)\n",
        "                print(\"\\n========= saved checkpoint =========\")\n",
        "\n",
        "        self.current_episode += 1\n",
        "        # annealing\n",
        "        if self.current_episode < self.epsilon_anneal_episodes:\n",
        "            self.epsilon -= (self.epsilon_anneal_from - self.epsilon_anneal_to) / float(self.epsilon_anneal_episodes)\n",
        "    \n",
        "    def get_mean_loss(self):\n",
        "      mean_loss = 0.\n",
        "      if len(self.loss) != 0:   \n",
        "          mean_loss = sum(self.loss) / len(self.loss)\n",
        "      self.loss = []\n",
        "      return mean_loss\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHuITVpD0nAN",
        "colab_type": "text"
      },
      "source": [
        "## Setup configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9jFzEnr1zt9",
        "colab_type": "text"
      },
      "source": [
        "### Vocab\n",
        "Upload vocab.txt file`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfc78DzULeIf",
        "colab_type": "code",
        "outputId": "fc7bf8be-7b2f-4bb2-e6bc-687321568909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "if not os.path.isfile('./vocab.txt'):\n",
        "    uploaded = files.upload()\n",
        "    # Upload vocab.txt\n",
        "    for fn in uploaded.keys():\n",
        "        print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "else:\n",
        "    print(\"Vocab already uploaded!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab already uploaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhn6sd_gLvC-",
        "colab_type": "code",
        "outputId": "e38a7ec2-bd58-4c69-c732-d924478bb18b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!head vocab.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\n",
            "\"\n",
            "#\n",
            "$\n",
            "%\n",
            "&\n",
            "'\n",
            "'a\n",
            "'d\n",
            "'ll\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWAhh8mL2WCf",
        "colab_type": "text"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufN2JE2f1eTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./config.yaml', 'w') as config:\n",
        "    config.write(\"\"\"general:\n",
        "  discount_gamma: 0.7\n",
        "  random_seed: 42\n",
        "  use_cuda: True  # disable this when running on machine without cuda\n",
        "\n",
        "  # replay memory\n",
        "  replay_memory_capacity: 100000  # adjust this depending on your RAM size\n",
        "  replay_memory_priority_fraction: 0.5\n",
        "  update_per_k_game_steps: 8\n",
        "  replay_batch_size: 32\n",
        "\n",
        "  # epsilon greedy\n",
        "  epsilon_anneal_episodes: 60  # -1 if not annealing\n",
        "  epsilon_anneal_from: 1.0\n",
        "  epsilon_anneal_to: 0.2\n",
        "\n",
        "checkpoint:\n",
        "  experiment_tag: 'starting-kit'\n",
        "  model_checkpoint_path: '/gdrive/My Drive/Masters/TextWorld/models'\n",
        "  load_pretrained: False  # during test, enable this so that the agent load your pretrained model\n",
        "  pretrained_experiment_tag: 'starting-kit'\n",
        "  save_frequency: 100\n",
        "\n",
        "training:\n",
        "  batch_size: 10\n",
        "  nb_epochs: 100\n",
        "  max_nb_steps_per_episode: 100  # after this many steps, a game is terminated\n",
        "  optimizer:\n",
        "    step_rule: 'adam'  # adam\n",
        "    learning_rate: 0.001\n",
        "    clip_grad_norm: 5\n",
        "\n",
        "model:\n",
        "  embedding_size: 32\n",
        "  encoder_rnn_hidden_size: [62]\n",
        "  action_scorer_hidden_dim: 64\n",
        "  dropout_between_rnn_layers: 0.\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je184JvpCfos",
        "colab_type": "text"
      },
      "source": [
        "### Mount drive to load games\n",
        "\n",
        "Notebook takes sample games from google drive(requires authentication).\n",
        "\n",
        "To train the agent with games, upload archive with them in google drive and fix the path to the archive inside drive below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XugC3fAz9hpB",
        "colab_type": "code",
        "outputId": "1b4cfd93-a3b5-4cc1-d007-b9af4ed7b457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc8uotOGFIVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put your path of the games archive.\n",
        "# !ls -1 | grep -q '^.*\\.ulx$' || tar -xzvf '/gdrive/My Drive/simple_games.tgz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOV193DaBuEF",
        "colab_type": "code",
        "outputId": "6fc0aa8f-3cd0-4e01-b5af-72247d729886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!ls -l ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 164\n",
            "-rw-r--r-- 1 root root   1048 Jul  1 18:15 config.yaml\n",
            "drwxr-xr-x 1 root root   4096 Jun 18 16:14 sample_data\n",
            "-rw-r--r-- 1 root root 158830 Jul  1 10:16 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hLhnwCACncb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_sample_games = '/gdrive/My Drive/Masters/TextWorld/sample_games'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_P2f82lPnQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_additional_infos() -> EnvInfos:\n",
        "    \"\"\"\n",
        "    Returns what additional information should be made available at each game step.\n",
        "\n",
        "    Requested information will be included within the `infos` dictionary\n",
        "    passed to `CustomAgent.act()`. To request specific information, create a\n",
        "    :py:class:`textworld.EnvInfos <textworld.envs.wrappers.filter.EnvInfos>`\n",
        "    and set the appropriate attributes to `True`. The possible choices are:\n",
        "\n",
        "    * `description`: text description of the current room, i.e. output of the `look` command;\n",
        "    * `inventory`: text listing of the player's inventory, i.e. output of the `inventory` command;\n",
        "    * `max_score`: maximum reachable score of the game;\n",
        "    * `objective`: objective of the game described in text;\n",
        "    * `entities`: names of all entities in the game;\n",
        "    * `verbs`: verbs understood by the the game;\n",
        "    * `command_templates`: templates for commands understood by the the game;\n",
        "    * `admissible_commands`: all commands relevant to the current state;\n",
        "\n",
        "    In addition to the standard information, game specific information\n",
        "    can be requested by appending corresponding strings to the `extras`\n",
        "    attribute. For this competition, the possible extras are:\n",
        "\n",
        "    * `'recipe'`: description of the cookbook;\n",
        "    * `'walkt\n",
        "\n",
        "    hrough'`: one possible solution to the game (not guaranteed to be optimal);\n",
        "\n",
        "    Example:\n",
        "        Here is an example of how to request information and retrieve it.\n",
        "\n",
        "        >>> from textworld import EnvInfos\n",
        "        >>> request_infos = EnvInfos(description=True, inventory=True, extras=[\"recipe\"])\n",
        "        ...\n",
        "        >>> env = gym.make(env_id)\n",
        "        >>> ob, infos = env.reset()\n",
        "        >>> print(infos[\"description\"])\n",
        "        >>> print(infos[\"inventory\"])\n",
        "        >>> print(infos[\"extra.recipe\"])\n",
        "\n",
        "    Notes:\n",
        "        The following information *won't* be available at test time:\n",
        "\n",
        "        * 'walkthrough'\n",
        "    \"\"\"\n",
        "    request_infos = EnvInfos()\n",
        "    request_infos.description = True\n",
        "    request_infos.inventory = True\n",
        "    request_infos.entities = True\n",
        "    request_infos.verbs = True\n",
        "    request_infos.extras = [\"recipe\", \"walkthrough\"]\n",
        "    return request_infos\n",
        "  \n",
        "def gather_entites(game_files):\n",
        "    requested_infos = select_additional_infos()\n",
        "    _validate_requested_infos(requested_infos)\n",
        "\n",
        "    env_id = textworld.gym.register_games(game_files, requested_infos,\n",
        "                                          max_episode_steps=1,\n",
        "                                          name=\"training\")\n",
        "\n",
        "    env_id = textworld.gym.make_batch(env_id, batch_size=1, parallel=True)\n",
        "    env = gym.make(env_id)\n",
        "    game_range = range(len(game_files))\n",
        "    entities = set()\n",
        "    verbs = set()\n",
        "    for game_no in game_range:\n",
        "        obs, infos = env.reset()\n",
        "        env.skip()\n",
        "        entities |= { a for i in infos['entities'] for a in i }\n",
        "        verbs |= { a for i in infos['verbs'] for a in i }\n",
        "    return list(entities), list(verbs)\n",
        "    \n",
        "    \n",
        "def make_vocab(games):\n",
        "    entities, verbs = gather_entites(games)\n",
        "    \n",
        "    with open(\"./vocab.txt\") as f:\n",
        "        word_vocab = f.read().split(\"\\n\")\n",
        "        \n",
        "    #########################\n",
        "    batch_size = len(verbs)\n",
        "    verbs_word_list = verbs\n",
        "    noun_word_list, adj_word_list = [], []\n",
        "    tmp_nouns, tmp_adjs = [], []\n",
        "    for name in entities:\n",
        "        split = name.split()\n",
        "        tmp_nouns.append(split[-1])\n",
        "        if len(split) > 1:\n",
        "            tmp_adjs.append(\" \".join(split[:-1]))\n",
        "    noun_word_list = list(set(tmp_nouns))\n",
        "    adj_word_list = list(set(tmp_adjs))\n",
        "\n",
        "    word2id = { word: idx for idx, word in enumerate(word_vocab) }\n",
        "    \n",
        "    verb_mask = np.zeros((len(word_vocab),), dtype=\"float32\")\n",
        "    noun_mask = np.zeros((len(word_vocab),), dtype=\"float32\")\n",
        "    adj_mask = np.zeros((len(word_vocab),), dtype=\"float32\")\n",
        "  \n",
        "    for w in verbs_word_list:\n",
        "        if w in word2id:\n",
        "            verb_mask[word2id[w]] = 1.0\n",
        "    for w in noun_word_list:\n",
        "        if w in word2id:\n",
        "            noun_mask[word2id[w]] = 1.0\n",
        "    for w in adj_word_list:\n",
        "        if w in word2id:\n",
        "            adj_mask[word2id[w]] = 1.0\n",
        "\n",
        "    second_noun_mask = copy.copy(noun_mask)\n",
        "    second_adj_mask = copy.copy(adj_mask)\n",
        "    second_noun_mask[:, self.EOS_id] = 1.0\n",
        "    adj_mask[:, self.EOS_id] = 1.0\n",
        "    second_adj_mask[:, self.EOS_id] = 1.0\n",
        "    word_masks_np = [verb_mask, adj_mask, noun_mask, second_adj_mask, second_noun_mask]\n",
        "    \n",
        "    return word_vocab, word2id, word_masks_np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2HzAOxlCPDNI",
        "outputId": "09d26f8e-71d7-4c3e-cb61-12b336d9d69a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# List of additional information available during evaluation.\n",
        "AVAILABLE_INFORMATION = EnvInfos(\n",
        "    description=True, inventory=True,\n",
        "    max_score=True, objective=True, entities=True, verbs=True,\n",
        "    command_templates=True, admissible_commands=True,\n",
        "    has_won=True, has_lost=True,\n",
        "    extras=[\"recipe\"]\n",
        ")\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "def _validate_requested_infos(infos: EnvInfos):\n",
        "    msg = \"The following information cannot be requested: {}\"\n",
        "    for key in infos.basics:\n",
        "        if not getattr(AVAILABLE_INFORMATION, key):\n",
        "            raise ValueError(msg.format(key))\n",
        "\n",
        "    for key in infos.extras:\n",
        "        if key not in AVAILABLE_INFORMATION.extras:\n",
        "            raise ValueError(msg.format(key))\n",
        "            \n",
        "def get_index(game_no, stats):\n",
        "    return \"G{}_{}\".format(game_no, stats)\n",
        "  \n",
        "def save_to_csv(epoch_no, stats_df, states, loss, col):\n",
        "  filename = '/gdrive/My Drive/Masters/TextWorld/stats/game_' + str(col) + '.csv'\n",
        "  log_df = pd.DataFrame(columns=['epoch','score', 'steps', 'loss', 'state'])\n",
        "  log_df.loc[0,'epoch'] = epoch_no\n",
        "  log_df.loc[0,'score'] = stats_df[get_index(col, 'sc')]['avr']\n",
        "  log_df.loc[0,'steps'] = stats_df[get_index(col, 'st')]['avr']\n",
        "  log_df.loc[0,'loss'] = loss[col]\n",
        "  log_df.loc[0, 'state'] = states[col]\n",
        "  if not os.path.isfile(filename):\n",
        "     log_df.to_csv(filename, header=['epoch','score', 'steps', 'loss', 'state'])\n",
        "  else: # else it exists so append without writing the header\n",
        "     log_df.to_csv(filename, mode='a', header=False)\n",
        "            \n",
        "def print_epoch_stats(epoch_no, stats):\n",
        "    print(\"\\n\\nEpoch: {:3d}\".format(epoch_no))\n",
        "    steps, scores, loss, states = stats[\"steps\"], stats[\"scores\"], stats[\"loss\"], stats[\"state\"]\n",
        "    games_cnt, parallel_cnt = len(steps), len(steps[0])\n",
        "    columns = [ get_index(col, st) for col in range(games_cnt) for st in ['st', 'sc']]\n",
        "    stats_df = pd.DataFrame(index=list(range(parallel_cnt)) + [\"avr\", \"loss\"], columns=columns)\n",
        "        \n",
        "    for col in range(games_cnt):\n",
        "        for row in range(parallel_cnt):\n",
        "            stats_df[get_index(col, 'st')][row] = steps[col][row]\n",
        "            stats_df[get_index(col, 'sc')][row] = scores[col][row]\n",
        "        stats_df[get_index(col, 'sc')]['avr'] = stats_df[get_index(col, 'sc')].mean()\n",
        "        stats_df[get_index(col, 'st')]['avr'] = stats_df[get_index(col, 'st')].mean()\n",
        "        stats_df[get_index(col, 'sc')]['loss'] = \"{:.5f}\".format(loss[col])\n",
        "        stats_df[get_index(col, 'st')]['loss'] = states[col]\n",
        "        \n",
        "        save_to_csv(epoch_no, stats_df, states, loss, col)\n",
        "    print(stats_df)\n",
        "\n",
        "def train(game_files):\n",
        "    requested_infos = select_additional_infos()\n",
        "    \n",
        "    agent = CustomAgent()\n",
        "    env_id = textworld.gym.register_games(game_files, requested_infos,\n",
        "                                          max_episode_steps=agent.max_nb_steps_per_episode,\n",
        "                                          name=\"training\")\n",
        "\n",
        "    env_id = textworld.gym.make_batch(env_id, batch_size=agent.batch_size, parallel=True)\n",
        "    print(\"ENVID: {}\".format(env_id))\n",
        "\n",
        "    print(\"Making {} parallel environments to train on them\\n\".format(agent.batch_size))\n",
        "    env = gym.make(env_id)\n",
        "\n",
        "    max_score = -1\n",
        "    game_range = range(len(game_files))\n",
        "    for epoch_no in range(1, agent.nb_epochs + 1):\n",
        "        stats = {\n",
        "            \"scores\": [],\n",
        "            \"steps\": [],\n",
        "            \"loss\": [],\n",
        "            \"state\": []\n",
        "        }\n",
        "        \n",
        "        for game_no in tqdm(game_range):\n",
        "            obs, infos = env.reset()\n",
        "            imitate = random.random() > 0.7\n",
        "            agent.train(imitate)\n",
        "\n",
        "            scores = [0] * len(obs) \n",
        "            dones = [False] * len(obs)\n",
        "            steps = [0] * len(obs)\n",
        "            while not all(dones):\n",
        "                # Increase step counts.\n",
        "                steps = [step + int(not done) for step, done in zip(steps, dones)]\n",
        "                commands = agent.act(obs, scores, dones, infos)\n",
        "                obs, scores, dones, infos = env.step(commands)\n",
        "\n",
        "            # Let the agent knows the game is done.\n",
        "            agent.act(obs, scores, dones, infos)\n",
        "  \n",
        "            stats[\"scores\"].append(scores)\n",
        "            stats[\"steps\"].append(steps)\n",
        "            stats[\"loss\"].append(agent.get_mean_loss())\n",
        "            stats[\"state\"].append(\"imitate\" if imitate else \"agent\")\n",
        "            \n",
        "        #score = sum(stats[\"scores\"]) / agent.batch_size\n",
        "        #steps = sum(stats[\"steps\"]) / agent.batch_size\n",
        "        \n",
        "        print_epoch_stats(epoch_no, stats)\n",
        "    torch.save(agent.model, './agent_model.pt')\n",
        "    return\n",
        "          \n",
        "\n",
        "game_dir = path_to_sample_games\n",
        "games = []\n",
        "if os.path.isdir(game_dir):\n",
        "    games += glob.glob(os.path.join(game_dir, \"*.ulx\"))\n",
        "print(\"{} games found for training.\".format(len(games)))\n",
        "\n",
        "if len(games) != 0:\n",
        "    train(games)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 games found for training.\n",
            "LSTM_DQN(\n",
            "  (word_embedding): Embedding(\n",
            "    (embedding_layer): Embedding(20208, 32, padding_idx=0)\n",
            "  )\n",
            "  (encoder): FastUniLSTM(\n",
            "    (rnns): ModuleList(\n",
            "      (0): LSTM(32, 62)\n",
            "    )\n",
            "  )\n",
            "  (action_scorer_shared): Linear(in_features=62, out_features=64, bias=True)\n",
            "  (action_scorers): ModuleList(\n",
            "    (0): Linear(in_features=64, out_features=20208, bias=False)\n",
            "    (1): Linear(in_features=64, out_features=20208, bias=False)\n",
            "    (2): Linear(in_features=64, out_features=20208, bias=False)\n",
            "    (3): Linear(in_features=64, out_features=20208, bias=False)\n",
            "    (4): Linear(in_features=64, out_features=20208, bias=False)\n",
            "  )\n",
            ")\n",
            "total number of parameters: 7141056\n",
            "number of trainable parameters: 7141056\n",
            "ENVID: batch10-tw-training-v30\n",
            "Making 10 parallel environments to train on them\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "imitate\n",
            "take red hot pepper from counter\n",
            "imitate\n",
            "prepare meal\n",
            "imitate\n",
            "eat meal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|     | 1/2 [00:02<00:02,  2.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "imitate\n",
            "eat meal\n",
            "\n",
            "========= saved checkpoint =========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|| 2/2 [00:15<00:00,  5.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:   1\n",
            "        G0_st    G0_sc  G1_st    G1_sc\n",
            "0           3        3    100        0\n",
            "1           3        3    100        0\n",
            "2           3        3    100        0\n",
            "3           3        3    100        0\n",
            "4           3        3    100        0\n",
            "5           3        3    100        0\n",
            "6           3        3    100        0\n",
            "7           3        3    100        0\n",
            "8           3        3    100        0\n",
            "9           3        3    100        0\n",
            "avr         3        3    100        0\n",
            "loss  imitate  0.00000  agent  0.24636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|     | 1/2 [00:12<00:12, 12.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "imitate\n",
            "open fridge\n",
            "imitate\n",
            "take yellow bell pepper from fridge\n",
            "imitate\n",
            "take yellow potato from counter\n",
            "imitate\n",
            "take knife from table\n",
            "imitate\n",
            "slice yellow bell pepper with knife\n",
            "imitate\n",
            "drop knife\n",
            "imitate\n",
            "take knife\n",
            "imitate\n",
            "dice yellow potato with knife\n",
            "imitate\n",
            "drop knife\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|| 2/2 [00:14<00:00,  9.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "imitate\n",
            "prepare meal\n",
            "imitate\n",
            "eat meal\n",
            "imitate\n",
            "eat meal\n",
            "\n",
            "\n",
            "Epoch:   2\n",
            "      G0_st    G0_sc    G1_st    G1_sc\n",
            "0       100        0       11        6\n",
            "1       100        0       11        6\n",
            "2       100        0       11        6\n",
            "3       100        0       11        6\n",
            "4       100        0       11        6\n",
            "5       100        0       11        6\n",
            "6       100        0       11        6\n",
            "7       100        0       11        6\n",
            "8       100        0       11        6\n",
            "9       100        0       11        6\n",
            "avr     100        0       11        6\n",
            "loss  agent  0.19921  imitate  0.18856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|     | 1/2 [00:12<00:12, 12.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|| 2/2 [00:25<00:00, 12.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:   3\n",
            "      G0_st    G0_sc  G1_st    G1_sc\n",
            "0       100        0    100        0\n",
            "1       100        0    100        0\n",
            "2       100        0    100        0\n",
            "3       100        0    100        0\n",
            "4       100        0    100        0\n",
            "5       100        0    100        0\n",
            "6       100        0    100        0\n",
            "7       100        0    100        0\n",
            "8       100        0    100        0\n",
            "9       100        0    100        0\n",
            "avr     100        0    100        0\n",
            "loss  agent  0.17891  agent  0.17084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|     | 1/2 [00:12<00:12, 12.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|| 2/2 [00:25<00:00, 12.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:   4\n",
            "      G0_st    G0_sc  G1_st    G1_sc\n",
            "0       100        0    100        0\n",
            "1       100        0    100        0\n",
            "2       100        0    100        0\n",
            "3       100        0    100        0\n",
            "4       100        0    100        0\n",
            "5       100        0    100        0\n",
            "6       100        0    100        0\n",
            "7       100        0    100        0\n",
            "8       100        0    100        0\n",
            "9       100        0    100        0\n",
            "avr     100        0    100        0\n",
            "loss  agent  0.14734  agent  0.08408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|     | 1/2 [00:12<00:12, 12.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|| 2/2 [00:26<00:00, 12.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:   5\n",
            "      G0_st    G0_sc  G1_st    G1_sc\n",
            "0       100        0    100        0\n",
            "1       100        0    100        0\n",
            "2       100        0    100        0\n",
            "3       100        0    100        0\n",
            "4       100        0    100        0\n",
            "5       100        0    100        0\n",
            "6       100        0    100        0\n",
            "7       100        0    100        0\n",
            "8       100        0    100        0\n",
            "9       100        0    100        0\n",
            "avr     100        0    100        0\n",
            "loss  agent  0.08094  agent  0.10627\n",
            "tw-training-v30 closed\n",
            "tw-training-v30 closed\n",
            "tw-training-v30 closed\n",
            "tw-training-v30 closed\n",
            "tw-training-v30 closed\n",
            "tw-training-v30 closed\n",
            "tw-training-v30 closed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-309:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tw-training-v30 closed\n",
            "tw-training-v30 closed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-306:\n",
            "Process Process-308:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tw-training-v30 closed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "Process Process-303:\n",
            "Traceback (most recent call last):\n",
            "Process Process-305:\n",
            "Process Process-302:\n",
            "Traceback (most recent call last):\n",
            "Process Process-304:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Process Process-310:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Process Process-307:\n",
            "Process Process-301:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/textworld/gym/envs/batch_env.py\", line 20, in _child\n",
            "    command = pipe.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-00b927a83b70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-109-00b927a83b70>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(game_files)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Increase step counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mcommands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-36bdfd806633>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, scores, dones, infos)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0mword_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ranks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_description\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# list of batch x vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_indices_maxq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_maxQ_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ranks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_masks_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_indices_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_random_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ranks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_masks_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m           \u001b[0;31m# random number for epsilon greedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-36bdfd806633>\u001b[0m in \u001b[0;36mchoose_maxQ_command\u001b[0;34m(self, word_ranks, word_masks_np)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_qvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mword_qvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;31m#DEBUG:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m#idx = word_indices[j][i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDzTBU3IM3KY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8hIulvHzNce",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrIFg_ec1uvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -lh saved_models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfDuYKdNF-8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}